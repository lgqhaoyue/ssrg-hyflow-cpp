\documentclass[12pt,english]{report}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.1in}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}

% Uncomment for double-spaced document.
\renewcommand{\baselinestretch}{1}

% \usepackage{epsf}
\usepackage{graphicx}
\usepackage{listings}\lstset{
language=C++,                   % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it's 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
%frame=single,	                % adds a frame around the code
tabsize=4,		                % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
xleftmargin=0.7cm			    % sets to stop code cut into the margin
}
\usepackage{subfigure}

\makeatother
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{babel}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,		% false: boxed links; true: colored links
	linkcolor=black,          % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=black,      % color of file links
    urlcolor=black           % color of external links
}
\usepackage{rotating}
\usepackage{comment}
\usepackage{bbding}
\usepackage{float}
\usepackage{threeparttable}
\usepackage{amsthm}
\newtheorem{definition}{Definition}[section]



\begin{document}

\thispagestyle{empty}
\pagenumbering{roman}
\begin{center}

% TITLE
{\Large 
HyflowCPP : A Distributed Transactional Memory framework for C++
}

\vfill

Sudhanshu Mishra

\vfill

Thesis submitted to the Faculty of the \\
Virginia Polytechnic Institute and State University \\
in partial fulfillment of the requirements for the degree of

\vfill

Master of Science \\
in \\
Computer Engineering


\vfill

Binoy Ravindran, Chair \\
Robert P. Broadwater \\
Mark Jones


\vfill

January 28, 2013 \\
Blacksburg, Virginia

\vfill

Keywords: Distributed Transactional Memory, Transactional Framework, C++, Distributed Concurrency
\\
Copyright 2013, Sudhanshu Mishra

\end{center}

\pagebreak

\thispagestyle{empty}
\begin{center}

{\large
HyflowCPP : A Distributed Transactional Memory framework for C++
}
\vfill
Sudhanshu Mishra
\vfill
(ABSTRACT)
\vfill
\end{center}
The distributed transactional memory (DTM) abstraction aims to simplify the development of distributed concurrent programs. It frees programmers from the complicated and error-prone task of explicit concurrency control based on locks (e.g., deadlocks, livelocks, non-scalability, non-composability), which are aggravated in a distributed environment due to the complexity of multi-node concurrency. At its core, DTM's atomic section-based synchronization abstraction enables the execution of a sequence of multi-node object operations with the classical serializability property, which significantly increases the programmability of distributed systems. 

In this thesis, we present the first ever DTM framework for distributed concurrency control in C++, called \textit{HyflowCPP}. HyFlowCPP provides distributed atomic sections, and pluggable support for concurrency control algorithms, directory lookup protocols, contention management policies, and network communication protocols.
The framework uses the Transaction Forwarding Algorithm (or TFA) for concurrency control. While there exists implementations of TFA and other DTM concurrency control algorithms in Scala and Java, and concomitant DTM frameworks (e.g., HyflowJava, HyflowScala, D2STM, GenRSTM), HyflowCPP provides a uniquely distinguishing TFA/DTM implementation for C++. Also, HyflowCPP supports strong atomicity, transactional nesting models including closed and open nesting (supported using modifications to TFA), and checkpointing. 

We evaluated HyflowCPP through an experimental study that measured transactional throughput for a set of micro- and macro-benchmarks, and comparing with competitor DTM frameworks. Our results revealed that HyflowCPP achieves upto 600\% performance improvement over competitor Java DTM frameworks including D2STM, GenRSTM, HyflowScala and HyflowJava, which can be attributed to the competitors' JVM overhead and rudimentary networking support. Additionally, our experimental studies revealed that checkpointing achieves upto 100\% performance improvement over  flat nesting and 50\% over closed nesting. Open nesting based partial abort model achieves upto 140\% performance improvement over flat nesting and 90\% over closed nesting. 
%%BR: It is better to quote ON's improvement over CN, and also about CN versus ON. So I suggest re-phrasing the last sentence accordingly. 
%%BR: Done. 10:10PM, 1/19/2013. 
\vfill
This work is supported in part by US National Science Foundation under grants CNS 0915895, CNS 1116190, CNS 1130180, and CNS 1217385.. Any opinions, findings, and conclusions or recommendations expressed in this work are those of the author and do not necessarily reflect the views of the National Science Foundation.


\pagebreak

% Dedication and Acknowledgments are both optional
\chapter*{Dedication}

\begin{center}
I dedicate this thesis to my family and friends.

\textit{Without their support this would not have been possible}

\end{center}


\chapter*{Acknowledgments}

I would like to thank my advisor, Dr. Binoy Ravindran, for his help and guidance on both technical and personal topics. It has been an honor to work under him and I am highly thankful to him for his trust in me.

I would also like to thank Dr. Robert Broadwater and Dr. Mark Jones, for serving on my committee and providing their valuable feedback and direction. In addition, I would like to thank all of my colleagues
at the Systems Software Research lab. I would particularly like to thank Alex Turcu, Mohd. Saad and Aditya Dhoke for their support and encouragement. It was a pleasure to work with them and perform interesting research in area of Distributed Transactional Memory.

Finally, I would like to thank my family and friends for all the love and support they have given me, without which this thesis would not have been possible.

\tableofcontents
\pagebreak

\listoffigures
\pagebreak

%\listofalgorithms
%\pagebreak

\listoftables
\pagebreak

%\printnomenclature
%\pagebreak

\pagenumbering{arabic}
\pagestyle{myheadings}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																									%
%							CHAPTER 1	:	INTRODUCTION						%
%																									%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{chap:intro}
\markright{Chapter~\ref{chap:intro}.
Introduction
\hfill}

As Moore's law~\cite{schaller1997moore} is approaching towards the scaling limit, and processor clocks are hitting the power wall~\cite{Kish2002144}, chip manufacturers are progressively endowing the multi-core CPU architecture~\cite{Sun2010183}. Augmenting computing power by increasing processor clock rates and transistor scaling are no longer feasible~\cite{Thompson200620}. Consequently, application performance can only be improved by exposing greater concurrency in software that exploits the hardware parallelism~\cite{Mulitcore4563876}. 
%%BR: provide citation for each claim. 

Writing concurrent code is a daunting task for an ordinary programmer. Such code requires proper synchronization and co-ordination between different parallel tasks. Lock-based concurrency control is often used as the synchronization abstraction. Coarse-grain locking is simple to implement, and protects concurrent code by locking over a single, large critical section, but permits little concurrency. On the other hand, fine-grain locking decomposes a large critical section into a set of smaller critical sections to obtain greater concurrency. However, implementing fine grained locks is highly complex and prone to programmer errors such as deadlocks, livelocks, lock convoying, and priority inversion~\cite{DSM84877}. Additionally, lock-based synchronization is non-composable. For instance, a thread-safe collection may support atomic insertion or removal of elements using an internal lock. However, removing an element from one collection and inserting it into another in a thread safe manner  cannot be performed safely using the collection's internal lock. Rather, this requires additional synchronization support for locking multiple collections at the same time~\cite{Weihl:1989:LAP:63264.63518}. These issues with locks make concurrent code difficult to understand, program, modify, and maintain.
%%BR: provide citation for each locking method and all the claims. (See Herlihy/Shavit for appropriate references). 

Concurrency control has been well studied in the field of database systems, where \emph{transactions} have been a highly successful abstraction to make different operations access a database simultaneously without observing interference~\cite{gra88}. Transactions guarantee the four so-called ACID properties~\cite{gra88}: atomicity, consistency, isolation, and durability. This behavior is implemented by controlling access to shared data and undoing the actions of a transaction that did not complete successfully (i.e., roll back). Inspired by this success, transactional memory (TM) was proposed as an alternative model for accessing shared in-memory objects, without exposing locks in the programming interface, to avoid the drawbacks of locks. TM originated as a hardware solution (HTM)~\cite{herlihy93transactional},  was later extended in software, called STM~\cite{STMshavit95}, and subsequently to a hybrid model~\cite{damronFLLMN:HyTM:asplos:2006}. With TM, programmers organize code that read/write shared objects as transactions, which appear to execute atomically. Two transactions conflict if they access the same object and one access is a write. When that happens, a conflict resolution policy (e.g., contention manager~\cite{herlihy:stm-dynamic:podc:2003}) resolves the conflict by aborting one and allowing the other to commit, yielding (the illusion of) atomicity. Aborted transactions are re-started, often immediately. Thus, a transaction ends by either committing (i.e., its operations take effect), or by aborting (i.e., its operations have no effect). In addition to providing a simple programming model, TM provides performance comparable to lock-based synchronization~\cite{saha:mcrtstm2:popl:2006}. 


The difficulties of lock-based concurrency control are exacerbated in distributed systems, due to (additional) distributed versions of their centralized problem counterparts: distributed deadlocks, distributed livelocks, and distributed lock conveying are orders-of-magnitude harder to debug than in multiprocessors. Moreover, code composability is even more desirable in distributed systems due to the difficulties of reasoning about distributed concurrency, and the need to support replication or incorporate backup mechanisms that are needed to cope with failures (one of the raison d'\^etre for building distributed systems). The success of multiprocessor TM has therefore similarly motivated research on  distributed TM. 
%%BR: "Raison d'tre" is a French expression for "reason for existence". Be sure to get the latex formatting right for this. 

Distributed TM (DTM) can be classified based on the system architecture: cache-coherent DTM (cc DTM~\cite{HerlihyS07,zha09,zha093,zha092,Saad:2011:HHP:1996130.1996167,
kr-sss10,hyflow-transact11,snake,ntfa-11},  in which a set of nodes communicate with each other by message-passing links over a communication network, and a cluster model (cluster DTM)~\cite{boc08,manassiev:cluster-tm:2006,kot08,couceiro:d2stm:prdc:2009,romano:distribute-tm-clusters:lncs:2009,romano:cloud-tm:sigops-review:2010}, in which a group of linked computers works closely together to form a single computer. The most important difference between the two is communication cost. cc DTM assumes a \textit{metric-space} network (i.e., the communication cost 
between nodes form a metric), whereas cluster DTM differentiates between local cluster memory and remote memory at other clusters. 
cc DTM uses a cache-coherence protocol to locate and move objects in the network, satisfying object consistency properties. 

Similar to multiprocessor TM, DTM provides a simple distributed programming model (e.g., locks are entirely precluded in the interface), and performance comparable to distributed lock-based concurrency control~\cite{couceiro:d2stm:prdc:2009,kr-sss10,boc08,romano:distribute-tm-clusters:lncs:2009,romano:cloud-tm:sigops-review:2010,manassiev:cluster-tm:2006,zha09,zha092,zha093,Saad:2011:HHP:1996130.1996167,kr-sss10,hyflow-transact11,snake}. 

To define the thesis problem space and describe the thesis's research contributions, we now discuss key dimensions of the TM problem space. 

\section{The Transactional Memory Problem Space}

The dimensions of the TM problem space that are of interest to this thesis include memory consistency models, concurrency control mechanisms, replication mechanisms, atomicity semantics, and partial abort models. We discuss each of these in the subsections that follow. 

\subsection{Memory Consistency Models}

A \textit{memory consistency} model~\cite{MemoryConsistency546611} is an agreement between the TM framework and the programmer that specifies the memory access rules. The rules ensure that the memory remains consistent and predictable after different memory operations. If programmer follows the rules, memory consistency is guaranteed by the framework. The memory operations are generally provided at a low level of abstraction such as read, write, or compare and swap. 
The memory consistency models that are typically used in TM and DTM frameworks include serializability~\cite{serializabilityFormal}, opacity~\cite{Guerraoui:2008:CTM:1345206.1345233}, and snapshot isolation~\cite{SnapShot:Berenson:1995:CAS:223784.223785}.
%%BR: opacity?
%%BR: Provide citations. 

\textit{Serializability} is a strong consistency requirement and requires that all transactions execute in  complete isolation i.e., concurrent transactional execution must be equivalent to sequential execution. Two or more transactions can execute at the same time only if the equivalence to a serial execution can be maintained. DTM works like D2STM~\cite{D2STM:5368778}, Cluster-STM~\cite{Bocchino:2008:STM:1345206.1345242}, and GenRSTM~\cite{GenRSTM:6038614} support serializability.
%%BR: Provide citations. 

\textit{Opacity} is a stronger consistency requirement in comparison to serializability. It not only requires are transactions to execute in isolation, but also prevents even non-committed transactions from accessing inconsistent states of system. DTM works like HyflowJava~\cite{Saad:2011:HHP:1996130.1996167} and HyflowScala~\cite{turcuhyflow2} provided opacity guarantee.

Strong consistency requirements like serializability and opacity can potentially cause lower performance.  Thus, in recent years, many researchers have focused on weaker consistency models, such as snapshot isolation (SI) and eventual consistency (EC)~\cite{Eventual:Vogels:2009:EC:1435417.1435432}.

With \textit{snapshot isolation}, a transaction takes an individual snapshot of the memory at the start of the transaction. When the transaction finishes, it commits only if the values of objects in its snapshot have not been updated by other committed transactions. DecentSTM~\cite{DecentSTM:5470446} work is a good example of snapshot isolation DTM work. \textit{Eventual consistency} guarantees that if no new updates are made for a period of time, all objects will be updated to their most recent values. Amazon Dyanmo~\cite{DeCandia:2007:DAH:1294261.1294281} is a nice example of EC based distributed concurrency product.

Even though weaker consistency models provide higher performance by relaxing the consistency requirement, it forces programmers to embrace the relaxed consistency models. Such models typically pose a greater challenge for ordinary programmers, as now they must understand all the subtleties of complex consistency properties to ensure program correctness. This is likely why, the serializability property, despite its negative impact on performance, has gained significant traction, especially in database settings. 

%%BR: Provide citations for each model, and example TM works that use them. 

\subsection{Concurrency Control Mechanisms} 

Concurrency control ensures that concurrent changes made by transactions do not violate the consistency model supported by the system -- i.e., it ensures the correctness of results generated through concurrent operations with respect to the consistency model at hand. Concurrency control mechanisms often use locks for a period of time. Broadly, there are two types of concurrency control mechanism: pessimistic concurrency control and optimistic concurrency control.

\textit{Pessimistic concurrency control} (PCC)~\cite{PessCon:1311029} is based on the premise that all object accesses will result in conflicts. Therefore, as soon as any object access is made in a transaction, locks are acquired on the object. Depending upon the lock type (e.g., shared, exclusive), an object may be shared with other transactions. The well known two phase locking protocol~\cite{2PL:lin1983basic} is an example of a PCC mechanism.  

\textit{Optimistic concurrency control} (OCC)~\cite{OptCon:Herlihy:1990:AVA:77643.77647} is based on the premise that conflicts are possible, but very rare. Therefore, no locks are acquired at the time of object access, and objects are read from, and written to without acquiring any lock. Before commit, a validation step is performed to check for any conflicting transaction. If a conflict is detected, the current transaction is aborted and retried. OCC generally involves  four steps:
\begin{enumerate}
\item Begin: marks the start of a transaction.
\item Modify: read and write operations are performed on an object.
\item Validate: verifies whether any conflicting updates were made on accessed objects.
\item Commit/Abort: based on validation, in this step, the transaction is committed or aborted.
\end{enumerate}   

%%BR: Provide citations for each mechanism. 

\subsection{Replication}

Replication is a commonly used technique in databases for fault-tolerance and improved concurrency~\cite{faultReplic:1022297}. 
%%BR: Provide citation
In the DTM setting, the classical replication solution can be viewed as a primary-backup model~\cite{budhiraja1993primary, goldring1994discussion}, where the primary object copy is used to update all backup copies. 
%%BR: Provide citation
In case of primary failure, the backup copies are used to service transactions. The switch from primary to backup copies creates many problems for the transactions in progress, at the time of failure. Many systems send log updates to backup copies, to allow in-progress transactions to continue from a certain logged state~\cite{ImpData840959}. In case of high update workload too much overhead for full replication renders primary-backup approach inefficient. In such conditions a partial replication~\cite{alonso1997partial} technique is used, which  replicates only some specific entries and attributes. The entries and attributes to be replicated are specified based on partial replication protocol. Another popular replication technique is state machine approach~\cite{Schneider93chapter7}. In the replicated state machine approach, one machine is the master and the others are slaves. Master receives all the requests from clients and executes them on all replicas in the same order ensuring identical state for each replica.
%%BR: Provide citations. 

Replication is also viewed as a way to increase performance through localization. Replication reduces the network communication cost for the distributed read transactions significantly as all the requested objects are available locally. Same cost advantage in not available for write transactions, as each write updates all the replicas of objects, which incurs additional messaging cost. In such cases partial replication can provide a intermediate approach for replication. In some cases partial replication with weaker consistency have already shown magnitudes of performance improvement for distributed transactions.~\cite{GMU:peluso2012scalability}    
%%BR: Discuss this further with a similar-length paragraph. 

\subsection{Atomicity Semantics}

Two important atomicity semantics studied in the TM literature include strong atomicity and weak atomicity. Strong atomicity~\cite{abadi2009transactional} ensures that all inconsistent states of the system are hidden from non-transactional code. 
%%BR: Provide citation
In essence, strong atomicity treats non-transactional code as executing in its own singleton atomic transaction. This behavior prevents non-transactional code from accessing any unprotected shared variables outside the transactional code. 

In contrast to strong atomicity, the weak atomicity~\cite{strongAtomicityblundell2006subtleties} property ensures that transactional code executes  atomically only with respect to other transactional code. 
%%BR: Provide citation
This means that, non-transactional code can still access shared variables in an inconsistent state. Stronger atomicity thus clearly promotes programmability: it helps the programmer by preventing any potentially buggy interleavings. 

\section{Partial Abort Models}

In TM and DTM, the two common techniques that have been studied to improve performance through partial transactional abort, instead of full transactional abort, include \textit{nesting}~\cite{moss2005nested} and \textit{checkpointing}~\cite{koskinen2008checkpoints}. 
%%BR: Cite each
A transaction is said to be nested when it encloses some other transaction. Nesting also promotes code composability when integrating with external libraries: it enables a transaction to invoke calls to a library that are transactional. Different transactional nesting models have been studied in the past:\emph{Flat}, \emph{Closed}~\cite{turcu2012closed}, and \emph{Open}~\cite{turcu2012open}. 
%%BR: Cite each

\textit{Flat} nesting is the simplest form of nesting, which simply ignores the existence of transactions in inner code. All operations are executed in the context of the outermost transaction. Aborting any inner transaction causes the parent transaction to abort. Thus, no partial rollback can be performed with this model. Flat nesting does not provide any performance improvement over non-nested transactions.

\textit{Closed} nesting allows inner transactions to abort individually. Abort of an inner transaction does not lead to abort of the parent transaction. However, inner transactions' commit are not visible outside the parent transaction. An inner transaction commits to the internal memory of its parent transaction, and the commit is visible outside of the parent transaction only when the parent transaction commits. 

\textit{Open} nesting uses the higher level of abstraction for memory access to avoid any false conflict  occurring at memory levels. It allows inner transactions to commit or abort individually, and their commits are visible globally to all other transactions. In case of abort of the outermost transaction, due to any fundamental conflict at higher abstractions, all the inner transactions are roll-backed via predefined compensating action for each inner transaction.

\begin{figure}
\subfigure[Flat nesting] {
\includegraphics[scale=0.6]{\string"eps/nesting-example-flat".pdf}
}
\subfigure[Closed nesting] {
\includegraphics[scale=0.6]{\string"eps/nesting-example-closed".pdf}
}
\subfigure[Open nesting] {
\includegraphics[scale=0.6]{\string"eps/nesting-example-open".pdf}
}
\subfigure[Checkpointing] {
\includegraphics[scale=0.22]{\string"eps/checkpointing-example".pdf}
}
\caption{Simple example showing the execution time-line for two transactions under different transactional nesting models and checkpointing.}
\label{Fig:Nesting_example}
\end{figure}

\textit{Checkpointing} does not view a transaction as a composition of multiple inner transactions, but as a process containing various states.~\cite{Checkpointing:1702129} 
%%BR: Provide citation. 
The idea is to save the transaction execution state at various points, called `checkpoints', and use them to partially rollback to a checkpointed state in case of a conflict. This allows the transaction to redo the work only for that part of the transaction in which the conflict occurs. The transaction execution state is generally saved using continuations~\cite{flanagan1993essence}. 
%In HyflowCpp we use \textit{setcontext} and \textit{getcontext} functions to save the execution states.     

Figure~\ref{Fig:Nesting_example}~\cite{Alex:ONTFA2367601} illustrates the difference between the various transactional nesting models and checkpointing using two transactions $T1$ and $T2$. In flat nesting, upon a conflict at a shared object between transactions $T1$ and $T2$, $T2$ must fully abort. $T2$ can later restart and commit, when the shared object is released at the end of $T1$'s execution. 

In case of closed nesting, $T2$'s inner transaction incurs an abort. But, it saves all other previously committed inner transactions from getting aborted. Inner transactions can continue as soon as the shared object is released by $T1$. 

With open nesting, the aborted inner transaction of $T2$ does not have to wait till the end of $T1$'s execution, as in closed nesting. Instead, it can continue as soon as $T1$'s inner transaction which uses the shared object commits and releases the object. 

With checkpointing, upon a conflict, $T2$ can resume from the last valid checkpoint and keep retrying until $T1$ releases the conflicting object at the end of its execution. 

\section{Thesis Contribution: a DTM Framework for C++}

Most of the existing DTM implementations are created in VM-based languages (e.g., Java, Scala). Examples include Cluster-STM~\cite{Bocchino:2008:STM:1345206.1345242}, D2STM~\cite{D2STM:5368778}, DiSTM~\cite{Kotselidis08distm:a}, GenRSTM~\cite{GenRSTM:6038614}, HyflowJava~\cite{Saad:2011:HHP:1996130.1996167}, and HyflowScala~\cite{turcuhyflow2}. A few DTM implementations such as DMV~\cite{Manassiev:2006:EDV:1122971.1123002} (C++), Cluster-STM~\cite{Bocchino:2008:STM:1345206.1345242} (C), and Sinfonia~\cite{Aguilera:2009:SNP:1629087.1629088} (C++)  were developed in C \& C++. However, DMV~\cite{Manassiev:2006:EDV:1122971.1123002} and Cluster-STM~\cite{Bocchino:2008:STM:1345206.1345242}  are implemented as proofs-of-concept for  DTM algorithms, and Sinfonia~\cite{Aguilera:2009:SNP:1629087.1629088} is designed as a backup and restore service, rather than as general purpose DTM frameworks. 


Many high-performance production systems are developed in C++, instead of VM-based languages, usually, to overcome the performance issues with VM-based languages~\cite{hundt2011loop}. Therefore, a C++ framework for DTM is highly desirable as that enables high performance C++ applications to use DTM concurrency control. Additionally, a C++ DTM framework can be useful to understand the trade-offs between different transactional nesting and checkpointing models, where execution states can be saved without significant overhead in C++~\cite{continuation:wiki}. 
%%BR: Provide a citation for this. 

Motivated by this observation, we design and implement the first ever DTM framework for C/C++: HyflowCPP. HyflowCPP has a modular architecture and provides pluggable support for DTM protocols, cache coherence protocols, contention management policies, and networking protocols. HyflowCPP provides a simple atomic section-based DTM interface that excludes locks. The current implementation uses the TFA algorithm~\cite{saad2011transactional}, which is a dataflow-based cc DTM protocol. 
%%BR: Cite it. 
Additionally, HyflowCPP supports strong atomicity, partial abort models including closed and open nesting, and checkpointing. None of the past DTM systems support all these, and in particular for C/C++. 

\begin{table}[htbp]
\centering%
\begin{threeparttable}[b]
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
\begin{sideways} Implementation \end{sideways} & \begin{sideways} Serializability \end{sideways} & \begin{sideways} Replication \end{sideways} & \begin{sideways} MultiVersioning \end{sideways} & \begin{sideways} Strong Atomicity \end{sideways}  & \begin{sideways} checkpointing \end{sideways} & \begin{sideways} Closed-Nesting \end{sideways} & \begin{sideways} Open-Nesting \end{sideways} &  Target Language \tabularnewline
\hline
DMV ~\cite{Manassiev:2006:EDV:1122971.1123002} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & C++ \tabularnewline
\hline 
Cluster-STM ~\cite{Bocchino:2008:STM:1345206.1345242} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \CheckmarkBold{} \tnote{1}& \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & C \& SQL \tabularnewline
\hline 
DiSTM ~\cite{Kotselidis08distm:a} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & Java \tabularnewline
\hline 
D2STM ~\cite{D2STM:5368778} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & Java \tabularnewline
\hline 
AGGRO ~\cite{AGGRO:5598236} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & Java \tabularnewline
\hline 
Sinfonia\tnote{2} ~\cite{Aguilera:2009:SNP:1629087.1629088} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & C++ \tabularnewline
\hline  
GenRSTM ~\cite{GenRSTM:6038614} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & Java \tabularnewline
\hline 
GTM ~\cite{sridharan2011scalable} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & Chapel \tabularnewline
\hline
HyflowJava ~\cite{Saad:2011:HHP:1996130.1996167} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \CheckmarkBold{} & \CheckmarkBold{} & Java \tabularnewline
\hline
HyflowScala ~\cite{turcuhyflow2} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & Scala \tabularnewline
\hline
Granola ~\cite{cowling2012granola} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{}  & \XSolidBold{} & Java \tabularnewline
\hline
HyflowCPP & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \CheckmarkBold{} & \CheckmarkBold{} & \CheckmarkBold{} & \CheckmarkBold{} & C++ \tabularnewline
\hline
Decent RSTM ~\cite{DecentSTM:5470446} & \XSolidBold{} & \CheckmarkBold{} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & Java \tabularnewline
\hline
GMU ~\cite{GMU:peluso2012scalability} & \XSolidBold{} & \CheckmarkBold{} & \CheckmarkBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & \XSolidBold{} & Java \tabularnewline
\hline
\end{tabular}
\begin{tablenotes}
\item [1] Supported via programming restriction
\item [2] Developed as service for backup and restore
\end{tablenotes}
\end{threeparttable}
\caption{Comparison of DTM implementations and HyflowCPP's unique features.}
\label{tbl:stmComp}
\end{table}

Table~\ref{tbl:stmComp} summarizes HyflowCpp's contribution. Each row of the table describes a DTM implementation, and each column describes a DTM property. Thus, the table entries describe the features supported by the different DTMs, illustrating HyflowCPP's uniquely distinguishing features. 


We evaluated HyflowCPP through a set of experimental studies that measured transactional throughput on an array of micro and macro-benchmarks, and compared with past VM-based DTM systems such as GenRSTM, DecentSTM, HyflowJava and HyflowScala.
%%BR: List all your competitors. 
Key result from our experimental studies is that, HyflowCPP outperforms its nearest competitor HyflowScala
%%BR: List all your competitors. 
by as much as 6 times. Other competitor like GenRSTM, DecentSTM, and HyflowJava perform even worse in comparison to HyflowCPP.
%%BR: Complete it. (This is your baseline result for the flat nesting case.)
Additionally, we show that HyflowCPP's checkpointing-based partial rollback mechanism outperforms flat nesting by as much as 100\% and closed nesting by as much as 50\%. Open nesting based partial rollback approach outperforms flat nesting by as much as 140\% and closed nesting by as much as 90\%.
%%BR: Complete it.
These trends are similar to past DTM studies on nesting, 
%%BR: Cite Alex's papers. 
but our improvements (of checkpointing over closed nesting and open nesting over closed nesting) are  higher, which can be attributed to efficient design and C/C++ low overhead mechanisms. 
%%BR: Is that true? By design we are saving least possible data

\section{Thesis Organization}

The rest of the thesis is organized as follows: Chapter~\ref{chap:relWork} overviews past and related works in DTM, and contrast them with the thesis's problem space. Chapter~\ref{chap:progInterface} details the programming model of HyflowCPP. Chapter~\ref{chap:sysArch} describes HyflowCPP's architecture. Chapter~\ref{chap:algorithm} discusses the TFA algorithm and its extension to different transactional nesting and checkpointing models. We report our experimental results in Chapter~\ref{chap:expResults}. Finally, we conclude the thesis in Chapter~\ref{chap:conclusion}.


%%BR: STOPPED HERE. Thursday, 1/17/2013, 9:38PM. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																									%
%							CHAPTER 2	:	Related work						%
%																									%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}\label{chap:relWork}
\markright{Chapter~\ref{chap:relWork}.
Related Work
\hfill}

In this chapter we survey the past and related work focusing on the problem of distributed transactional memory. For our discussion purpose, we classify the past and current work in DTM based on different transactional properties:

\begin{enumerate}
\item Serializability
\item Non-Serializability 
\item Replication 
\item Strong Atomicity
\item Transaction Models
\end{enumerate}

In each section we describe the various research works and related transactional property definitions. In last section we also describe recently developed various unconventional systems.

\section{Serializable DTM implementations}

In previous chapter~\ref{chap:intro}, we have intuitively described \textit{serializability} property. Here we provide a formal definition of serializability. 

\begin{definition}[\textbf{Serializability}]
A set of transactions executes serially if each transaction executes its write operation before the next transaction executes its read operation. That is, the transactions are in no way interleaved. A serial execution of transactions preserves database consistency because each individual transaction preserves
database consistency. If an interleaved execution of transactions produces the same effect as a serial execution of those same transactions, then the execution is called serializable. Since a serial execution preserves consistency, a serializable execution also preserves consistency.~\cite{serializabilityFormal}
\end{definition}

In lock-based concurrency control implementation, serializability requires that locks to be acquired in certain range of execution. While in non-lock concurrency control, no lock is acquired, but if the system detects a concurrent transaction in progress it rollbacks.

One of the first work in DTM by Manassiev~\cite{Manassiev:2006:EDV:1122971.1123002}, supported the serializability. It introduced a novel page-level distributed concurrency control algorithm, called Distributed Multiversioning(DMV). DMV allows each node to keep a single local copy of the data items to read or write. At commit time page differences are broadcasted to all other replicas, and a transaction commits successfully upon receiving acknowledgments from all nodes. A central timestamp is employed, which allows only a single update transaction to commit at a time. Unfortunately, this requirement of transaction to acquire a cluster-wide unique token, which globally serializes the commit phases of transactions imposes considerable overhead and seriously hampers performance as later described by Kotselidis et. al.~\cite{Kotselidis08distm:a}.

Cluster-STM~\cite{Bocchino:2008:STM:1345206.1345242} work based on PGAS~\cite{PGAS:Programmin:Model} programming model supported serializibility. In Cluster-STM, the dataset is partitioned across the nodes and each data item is assigned a home node. Home node maintains the authoritative version of data item and synchronizes the accesses of conflicting remote transactions. Being based on PGAS~\cite{PGAS:Programmin:Model} programming model Cluster-STM does not distinguish transaction that execute in the same node from the transaction that execute on a different node and pays a heavy performance penalty for not exploiting shared memory for intra-node communication.

DiSTM ~\cite{Kotselidis08distm:a} work published in 2008 uses a distributed mutual exclusion mechanism to coordinate the commit of transactions. This mechanism ensures that no two conflicting transactions try to commit simultaneously. To provide distributed mutual exclusion this protocol grants lease to nodes on datasets, based on the their data access pattern, for each transaction commit. It allows transaction to escape performance penalty incurred by serialization cost in commit phase. However, it still become a bottleneck in contention intensive workloads. Also DiSTM suffers the scalability issues due to the single coordinating node performing lease establishment mechanism as the number of nodes increases. Due to this bottle neck DiSTM provides a dedicated node to perform lease establishment mechanism.

In 2009 Dependable Distributed Software Transactional Memory(D2STM)~\cite{D2STM:5368778} followed which utilized the atomic broadcast~\cite{Defago:2004:TOB:1041680.1041682} and bloom filter~\cite{Bloom:1970:STH:362686.362692} certification to achieve good performance in a replicated cluster. Replication of objects allows it to execute all read only transactions locally without incurring in any network communication overhead. For write transactions D2STM first validates it locally and aborts if required on basis of locally available information. After validating locally, replication manager encodes the transaction read-set in a bloom filter and atomically broadcasts it along with the transaction write-set. Even though atomic broadcast allowed the D2STM to support serializibility, it incurs high performance cost with increase in the number of nodes in cluster. Also, use of bloom filter requires the prior knowledge of transaction read-set to fine tune for reduced false positives. Same research group later came up with AGGRessively Optimistic concurrency control scheme(AGGRO)~\cite{AGGRO:5598236} to address dependability issue in DTM utilizing the replication. AGGRO propagates dependencies across uncommitted transactions in a serialization order compliant with the optimistic message delivery order provided by the optimistic atomic broadcast(OAB)~\cite{OAB:Pedone200379} service. Even though OAB allowed to improve performance, it also make it prone to saturation issue with the OAB group communication subsystem.  

In 2009 another work, namely, Sinfonia~\cite{Aguilera:2009:SNP:1629087.1629088} service came out which utilized the mini-transactions. The idea behind the mini-transactions was to send the transaction itself as a piggyback in first phase of two phase commit. All the transactions for which conditional value and update object exist on same node can be convert to a mini-transaction. Utilizing the mini-transaction Aguilera et. al. were able to reduce the network communication to a large extent and achieve good performance. Drawback of this approach is that we need to know the data accessed by transaction beforehand.

Cloud-TM~\cite{Romano:2010:CHC:1773912.1773914} work published in 2010 enumerated the features which can be useful to make DTM successful in providing concurrency solution over network cloud. They suggested to make DTM easily graspable by hiding complexities and make it capable to cope up with workload heterogeneity. They also make a point to maximizing locality and automatic resource provisioning for a high performance and adaptability of system. They asked DTM community to support durability to survive in failure prone cloud environment. 

In 2011, based on D2STM and AGGRO work Romano et. al. came up with a generic framework for Replicated Software Transactional Memories(GenRSTM) ~\cite{GenRSTM:6038614}. Goals of this framework was to simplify the development and testing of new replication protocols and STMs, provide high decoupling between the architecture building blocks, and support multiple implementations of the architecture building block. This framework enabled system administrators to seek optimal performance as a function of the workload/deployment scenario by reconfiguring the replicated STM middle-ware platform, in a transparent fashion for the
user level application. It simplified the development and evaluation of alternative replication protocols.

In same year Srinivas et. al. from Oak Ridge National Laboratory published a technical report~\cite{sridharan2011scalable} on language based software transactional memory for distributed memory systems. In Chapel~\cite{chapel:Language}, a general-purpose parallel language, they provided atomic semantics and pluggable compiler support for multiple DTM implementations. They also provided a prototype distributed STM implementation Global Transactional Memory 2 (GTM2) a enhancement over GTM~\cite{sridharan2009scalable} work published in 2009 based on remote procedure call(RPC) to provide DTM  support. In GTM2 simple RPC was improved with read versioning, deferred update, and eager acquire scheme.

Similar to GenRSTM, in 2011 Hyflow a Java framework~\cite{Saad:2011:HHP:1996130.1996167} for DTM was released for non-replication based systems. Later in 2012 a DTM framework in Scala language ~\cite{turcuhyflow2}  was released which showed improvement over previous framework. Both of these frameworks utilized TFA algorithm for there prototype implementation, which uses an asynchronous clockbased validation technique to ensure DTM transactional properties. HyflowCPP framework also uses TFA as its base transactional algorithm. We will describe TFA algorithm later in detail in Chapter~\ref{chap:algorithm}. All TFA based algorithm works HyflowJava, HyflowScala and HyflowCPP supported opacity a stronger property in comparison to serializability.   Formally opacity can be defined as described below:

\begin{definition}[\textbf{Opacity}]
A set of transactions executes serially if each transaction 
(1) all operations performed by every committed transaction
appear as if they happened at some single, indivisible point during
the transaction lifetime, (2) no operation performed by any
aborted transaction is ever visible to other transactions (including
live ones), and (3) every transaction always observes a consistent
state of the system.~\cite{Guerraoui:2008:CTM:1345206.1345233}
\end{definition}

Recently in 2012 Granola~\cite{cowling2012granola} work from MIT provided the support for the serializability. It divides the transactions in three different categories: Single-repository transactions executing using objects within a repository, coordinated distributed transactions executing using objects from more than one repositories, independent distributed transactions executing atomically across a set of repositories and commit independently. Coordinated distributed transactions follow the traditional two phase commit voting protocal and provide the current state of art performance. Meanwhile, single-repository transactions and independent distributed transactions using timestamp synchronization and no locking provide a high throughput.   

\section{Non-Serializable DTM implementations}

Many researchers have contented the serializability as a very strong criteria and used the weaker criteria to provide the high performance for DTM. Here, we describe some famous non-serializable consistency criteria and  related work in DTM. 

\textit{Snapshot isolation} is very famous consistency criteria often used in the database systems. Some researchers have developed the DTM systems based on this criteria. Snapshot isolation(SI) can be formally described as following:

\begin{definition}[\textbf{Snapshot Isolation}]
SI is defined by two properties: \textit{Snapshot-read} requires that a transaction T reads data from a snapshot which contains all updates committed before T starts (plus its own updates). \textit{Snapshot-write} requires that no two concurrent transactions may write the same object; that is, if two concurrent transactions both want to write the same data item only one of them will be allowed to commit.~\cite{SnapShot:Berenson:1995:CAS:223784.223785, SnapShot2:Lin:2009:SII:1538909.1538913}
\end{definition}

Snapshot-read is typically implemented via a multiversion system where read operations access previously committed versions. Multi-versioning is often used in databases to provides the concurrent access. It can be defined as following:
\begin{definition}[\textbf{Multi-Versioning}]
In multi-version concurrency control(MVCC), each write on an object $x$ creates a new copy of it. All copies of same object are given proper version based on history value. MVCC provides time-consistence views for the system. It allows the delayed read operations to execute successfully by providing the object version relevant to transaction's time-stamp. 
\end{definition}

Conflict detection for snapshot-write can be implemented via locking or via validation. In SI read and write skew anomalies can occur, which happens when two transactions concurrently read an overlapping data set, make disjoint updates, and finally concurrently commit. Neither of transaction see update performed by the other.

One of the first paper on weaker consistency model SI was DecentSTM~\cite{DecentSTM:5470446} in 2010.  DecentSTM algorithm keeps limited list of committed versions of all shared data and obtains lazily a consistent memory snapshot during a transaction’s execution. By choosing a version upon read, a transaction determines on which versions it depends. In fact with unlimited version history, a read only transaction would never have to abort, because it could always read a previous version that does not conflict with the data read so far. For coincidental commits DTM uses a voting based randomized consensus protocol. Using snapshot isolation do provide the higher performance in DecentSTM, but also adds up additional memory overhead of maintaining versioned objects.  

In 2011 Nuno et. al. in DiasSTM~\cite{dias2011efficient} came up with approach of static analysis of transactional code to provide serializable correctness to the snapshot consistency model based database systems. They suggested the methods to avoid read-write anomalies by automatically modifying the transaction code.

Genuine Multiversion Update-Serializable Partial Data Replication(GMU)~\cite{GMU:peluso2012scalability} work published in 2012 provided high performance using the consistency criterion Extended Update Serializability(EUS). Update serializability is a weaker consistency criteria and can be defines as follows:

\begin{definition}[\textbf{Update Serializability}]
A schedule $s$ over a set $T$ of transactions is update serializable (USR) iff each schedule $s'$ obtained from $s$ after deleting all but one read-only transaction is serializable. If there are no read-only transactions , then no transactions need be deleted~\cite{UpdateSerializability1986}.
\end{definition}

Extended Update Serializability (EUS)~\cite{EUS:HansdahPatnaik} allows concurrent read-only transactions to observe snapshots generated from different linear extensions of the history of update transactions.

At its heart GMU uses a distributed multiversion concurrency control scheme, a vector clock based synchronization algorithm, to track down data and causal dependency relations. It uses the partial replication to reduce the amount of network communication. 

\section{Replication}
Replication have been heavily used by DTM community to extract the transaction localization and to boost performance. For replication based transactional systems, consistency is generally defined using 1-copy serialization, which can be defined as described below:

\begin{definition}[\textbf{1-Copy Serializability}]
A concurrent execution of transactions in a replicated database is 1-copy-serializable if it is equivalent  to an ordering obtained when the transactions are performed sequentially in a single centralized database.~\cite{bornea2011one}
\end{definition}

As described earlier, first paper in DTM by Manassiev~\cite{Manassiev:2006:EDV:1122971.1123002} based on DMV algorithm, used data replication. For providing 1-copy serialization DMV used the global serialization time-stamp token, which proved costly approach to support 1-copy serialization in replication based systems. Later Dependable Distributed Software Transactional Memory(D2STM) system~\cite{D2STM:5368778} and AGGRO scheme~\cite{AGGRO:5598236} improved performance by replacing global time-stamp with atomic broadcast messages for all object replicas.  

Sinfonia~\cite{Aguilera:2009:SNP:1629087.1629088} service also supported replication. But it did not design the concurrency algorithm taking the replication in consideration, instead it used the primary-copy replication to recover in case of failure. 

DecentSTM~\cite{DecentSTM:5470446} used the full replication with random consensus concurrency protocol and provided the  snapshot consistency. It showed improvement using replication and Snapshot consistency but lacked any innovation in concurrency protocol. Later GMU~\cite{GMU:peluso2012scalability} presented a innovative concurrency protocol based on partial replication supporting Extended updated serializability and showed good performance improvement. 

\section{Strong Atomicity}
We have already introduced the strong atomicity informally in chapter~\ref{chap:intro}. Formally strong atomicity can be described as below:

\begin{definition}[\textbf{Strong Atomicity}] 
Strong atomicity is a transactional semantics that guarantees the atomicity between transactional and non-transactional code.
\end{definition}

Cluster-STM~\cite{Bocchino:2008:STM:1345206.1345242} supported the strong atomicity by implementing a programming restriction on user. It asked user to access each memory location always within transactions or always outside transactions, but did not provide any automatic support. DecentSTM~\cite{DecentSTM:5470446} programming model guaranteed strong atomicity and helped users to stop careless atomicity mistakes. HyflowScala~\cite{turcuhyflow2} and HyflowCpp also have en-build the strong atomicity using programming model.

\section{Partial Abort Models}

For performance improvement nesting techniques are widely used in database systems. In 1981 Moss~\cite{moss1981nested} first time described the nesting concept in a distributed database transactions. He extended two phase commit protocal~\cite{TwoPC:weikum1991principles} to support the nesting and proposed algorithms for distributed transaction management, object state restoration, and distributed deadlock detection. Later in 1983 Gracia~\cite{garcia1983using} et. al. extensively analyzed it in open nesting context using undo-logs transactions. 

Transaction nesting was first time introduced to STM in 2006 by  Moss and Hosking in ~\cite{moss2006nested}.They provided the semantics of transactional operations in terms of system states as a tuple of a transaction ID, a memory location, a read/write flag, and the value read or written. Later Moss ~\cite{moss2006open} further described the open-nesting as method to overcome false conflicts and improve concurrency. 

In same year Moravan et al. implemented nesting in logTM~\cite{moravan2006supporting} and demonstrated the speed-up 100\% for few benchmarks. In 2009 Agrawal et. al. ~\cite{agrawal2009safe} introduced the concept of transaction ownership by combining the closed and open nesting. Herlihy and Koskinen propose transactional boosting~\cite{herlihy2008transactional} for implementing highly concurrent transactional data structures, which internally implemented the open-nesting. Later Koskinen and Herlihy~\cite{koskinen2008checkpoints} suggested the checkpointing as an alternative to nesting. Recently, HyflowJava based work on closed nesting~\cite{turcu2012closed} and open nesting~\cite{turcu2012open} described the tradeoff of using nesting in DTM. 

\section{Unconventional Database Systems}
In recent times there have been lot of work on designing unconventional database system. With explosion in amount of data processed and stored in data warehouse, system administers are understanding the limitation of current database systems. Many works ~\cite{Stonebraker:2007:EAE:1325851.1325981}~\cite{harizopoulos2008oltp} have argued that current DBMS perform a poor job of CPU utilization and might require whole redesign. Works like HStore~\cite{HSTORE:kallman2008h} and Google Spanner~\cite{corbett2012spanner} have achieve high performance using new architectures based on replication and multi-versioning for database systems. 

\section{Summary}
HyflowCpp framework is implemented in C++ at API-level and focuses on non-replicated peer to peer distributed systems. Current default algorithm implementation, Transaction Forwarding Algorithm(TFA), provides the strong memory consistency. Using distributed clock, TFA is able to overcome any global serialization overhead. Single version objects without any replication helps TFA to reduce the amount of network messaging and enables it to scale without any network bottleneck.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																									%
%							CHAPTER 3	:	Programming Interface						%
%																									%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Programming Interface}\label{chap:progInterface}
\markright{Chapter~\ref{chap:progInterface}.
Programming Interface
\hfill}

In this chapter, we introduce the programming interface provided by HyflowCpp to execute the distributed atomic transactions. First we describe the basics of interface and then explain it by developing the list benchmark. HyflowCpp allows user to configure the benchmark and execution setting using a configuration file. User can also pass all these configuration variables as environment variables.

In a networked system objects are distributed over different nodes, therefore normal object reference can not be used. User is required to use a unique key to address a particular object anywhere in network. We provide a base class name ~\emph{HyflowObject} for every distributed object. Every distributed object must inherit this class. HyflowObject provides the getId() method which returns as unique key to access the object from anywhere in the network.

HyflowCpp performs object serialization using boost serialization library. User is required to follow it for proper packing and unpacking of object over the network. Objects created by user needs to register itself as inherited class of HyflowObject and register all the object field in boost serialize function to which it wish to be serializable over network.

For development of DTM applications HyflowCpp provides two transactional interfaces: 
\begin{enumerate}
\item Transaction support using Macros
\item Transaction support using Atomic class 
\end{enumerate}

\section{Transaction Support using Macros}

HyflowCpp provides standard atomic semantics using macros \emph{HYFLOW{\_}ATOMIC{\_}START} and \emph{HYFLOW{\_}ATOMIC{\_}END}. Figure~\ref{Fig:atomicConstr} shows how to utilize the these macro to execute any given part of code atomically and compares it to standard STM atomic semantics. 

\begin{figure}
\centering 
\begin{footnotesize}
\begin{minipage}[b]{0.45\linewidth}\centering
HyflowCpp atomic construct 
\begin{lstlisting}
HYFLOW_ATOMIC_START{
// Example of simple compare
// and swap operation
   value = Read(Address);
   if( value==myValue )
       Write(Address, myValue)
}HYFLOW_ATOMIC_END;
\end{lstlisting} 
\end{minipage} 
\begin{minipage}[b]{0.45\linewidth}\centering
Standard STM atomic construct
\begin{lstlisting}   					   
atomic{
// Example of simple compare
// and swap operation
   value = Read(Address);
   if( value==myValue )
       Write(Address, myValue)
}
\end{lstlisting}			  
\end{minipage}
\end{footnotesize}
\label{Fig:atomicConstr}\caption{Atomic Construct for HyflowCPP vs. Standard STM implementations}
\end{figure}

Any object in the network can be opened either in \emph{Read} or \emph{Write} mode. Once user requests the object, HyflowCpp fetches the object from its current location and copies to transactions read or write set depending of access type. For accessing any object user should macro \emph{HYFLOW{\_}FETCH(ID, IS{\_}READ)}. First argument to this macro is \emph{objectId} and second argument is access type \emph{true/false}, true for read, otherwise false.

For reading or writing the fetched object we provide two more macros \emph{HYFLOW{\_}ON{\_}READ(ID)} and \emph{HYFLOW{\_}ON{\_}WRITE(ID)}. \emph{HYFLOW{\_}ON{\_}READ} returns the user the constant reference pointer to HyflowObject, which can be used for read-only operations. For manipulating the object user again require to call \emph{HYFLOW{\_}ON{\_}WRITE}, which returns a normal reference pointer to object. User can also pass the object reference pointer itself in place of unique object key to retrieve the object.   

\emph{HYFLOW{\_}PUBLISH{\_}OBJECT(OBJ)} allows user to publish any locally created object on the network. But, such objects must inherit the HyflowObject class as a base type as discussed earlier. Similarly, \emph{HYFLOW{\_}PUBLISH{\_}DELETE(OBJ)} allows user to delete any object from the network.

Now using these macro's we can write the list benchmark as described in figure~\ref{Fig:listMacro}. In this figure we illustrate the list node addition and deletion atomically using HyflowCpp Macros. 
\begin{figure}
\centering
\begin{lstlisting}
class ListNode::HyflowObject {

void ListNode::addNode(int value) {
  HYFLOW_ATOMIC_START{
	std::string head="HEAD";
	HYFLOW_FETCH(head, false);

	ListNode* headNodeRead =  (ListNode*)HYFLOW_ON_READ(head);
	std::string oldNext = headNodeRead->getNextId();
	ListNode* newNode = new ListNode(value, ListBenchmark::getId());
	newNode->setNextId(oldNext);
	HYFLOW_PUBLISH_OBJECT(newNode);

	ListNode* headNodeWrite = (ListNode*)HYFLOW_ON_WRITE(head);
	headNodeWrite->setNextId(newNode->getId());
  } HYFLOW_ATOMIC_END;
}

void ListNode::deleteNode(int value) {
  HYFLOW_ATOMIC_START{
	ListNode* targetNode = NULL;
	std::string head("HEAD");
	std::string prev = head, next;

	HYFLOW_FETCH(head, true);
	targetNode = (ListNode*)HYFLOW_ON_READ(head);
	next = targetNode->getNextId();

	while(next.compare("NULL") != 0) {
	  HYFLOW_FETCH(next, true);
	  targetNode = (ListNode*)HYFLOW_ON_READ(next);
	  int nodeValue = targetNode->getValue();
	  if (nodeValue == value) {
		ListNode* prevNode = (ListNode*)HYFLOW_ON_WRITE(prev);
		ListNode* currentNode = (ListNode*)HYFLOW_ON_WRITE(next);
		prevNode->setNextId(currentNode->getNextId());
		HYFLOW_DELETE_OBJECT(currentNode);
		break;
	  }
	  prev = next;
	  next = targetNode->getNextId();
	}
  } HYFLOW_ATOMIC_END;
}

}
\end{lstlisting}
\caption{List benchmark using HyflowCPP Macros}
\label{Fig:listMacro}
\end{figure}

HyflowCpp also support the transaction \emph{Checkpointing} using macros. For minimum checkpointing overhead we support it in outermost transaction call. We allow user to checkpoint at any place using ~\emph{HYFLOW{\_}CHECKPOINT{\_}HERE}. User is also required to initiate the checkpointing at start of transaction using ~\emph{HYFLOW{\_}CHECKPOINT{\_}INIT}. Figure~\ref{Fig:bankCP} illustrates how checkpointing can be implemented in a simple bank transfer function. Note that how user is required to pass the current context instance to withdraw function. This requirement exist for all the functions which are called within atomic block and are require to be executed atomically. This additional argument passing will be removed once the compiler support is added to atomic block. 

\begin{figure}
\begin{minipage}[b]{0.9\linewidth}\centering
\begin{lstlisting}
void BankAccount::transfer(string Account1, string Account2, Money) {
	HYFLOW_ATOMIC_START {
		HYFLOW_CHECKPOINT_INIT;

		withdraw(Account1, Money, __context__);

		HYFLOW_CHECKPOINT_HERE;
	
		deposit(Account2, Money, __context__);
	}HYFLOW_ATOMIC_END;
}
\end{lstlisting}
\end{minipage}
\caption{Checkpointing in Bank transfer function}
\label{Fig:bankCP}
\end{figure}

\section{Transaction Support using Atomic class}

Atomic class interface is more involved and allows users to directly program against the HyflowCpp framework. Using the Atomic class interface user can directly interact with transaction context. Atomic class is template type class which must be type defined based on atomic function class return type. Atomic class function pointer ~\emph{atomically} is initialized by user with desired function, which is to be executed atomically. After initializing the function pointer value user can call the ~\emph{execute} method from atomic class to run the desired function atomically.

Atomic class also provides the function pointers to support advance nesting features like open-nesting. User can specify the ~\emph{onCommit} and ~\emph{onAbort} function for  ~\emph{HyflowObject} requiring the open nesting support. All these function pointers requires to follow a define argument set similar to libraries like pthread. Only hind side of using atomic class is that, checkpointing can not be performed on transaction using atomic class. Checkpoints are required to be created in outermost transactions and Atomic class executes the function pointer as an inner transaction. 

User can also write the benchmark directly without using any of Atomic class or Macros. This will require the user to directory manipulate the DirectoryManager for remote object access and ContextManager for transaction atomicity. It can be some time useful for debugging some internal framework issues, otherwise it is not a recommended method.

In Figure~\ref{Fig:listClassDelete} we re-write the same List benchmark functions using the atomic class. 

\begin{figure}
\centering
\begin{lstlisting}
class ListNode::HyflowObject {

void* deleteNodeAtomically(HyflowObject* self, void* args,
	HyflowContext* c, uint64_t* balance)
{
	int value = *((int*)args);
	ListNode* targetNode = NULL;
	std::string head("HEAD");
	std::string prev = head, next;

	HYFLOW_FETCH(head, true);
	
	targetNode = (ListNode*)HYFLOW_ON_READ(head);
	next = targetNode->getNextId();

	while(next.compare("NULL") != 0) {
		HYFLOW_FETCH(next, true);
		
		targetNode = (ListNode*)HYFLOW_ON_READ(next);
		
		int nodeValue = targetNode->getValue();
		if (nodeValue == value) {
		
			ListNode* prevNode = (ListNode*)HYFLOW_ON_WRITE(prev);
			
			ListNode* currentNode = (ListNode*)HYFLOW_ON_WRITE(next);
			
			prevNode->setNextId(currentNode->getNextId());
			
			HYFLOW_DELETE_OBJECT(currentNode);
			
			break;
		}
		prev = next;
		next = targetNode->getNextId();
	}
}

void ListNode::deleteNode(int value) {
	Atomic<uint64_t> atomiDelete;	
	
	atomicDelete.atomically = ListNode::deleteNodeAtomically;
	
	atomicDelete.execute(NULL, &value, NULL);
}

}
\end{lstlisting}
\caption{List benchmark using HyflowCPP Atomic Class}
\label{Fig:listClassDelete}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																									%
%							CHAPTER 4	:	System Architecture						%
%																									%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{System Architecture}\label{chap:sysArch}
\markright{Chapter~\ref{chap:sysArch}.
System Architecture
\hfill}

HyflowCpp is a distributed software transaction memory system at API level. It is implemented in C++ using object oriented programming paradigm. It is carefully design to provide pluggable support for different DTM protocols, cache-coherency protocols, and network libraries. All components are developed independent of each other and connected through well defined interfaces. Any individual component can be easily replaced or modified by writing a implementation compliant to the given component interface. 

Figure~\ref{Fig:HyflowCppArch} shows the architecture of a node in HyflowCpp. It it made of six modules: Transaction Interface Module, Transaction Validation Module, Object Access Module, Object serialization Module, Network Module, and Message Processing Module. We have already discussed about Transaction Interface Module in detail in chapter~\ref{chap:progInterface}. Now we will describe rest of modules in details in following sections.

\begin{figure}
\begin{minipage}[b]{0.9\linewidth}\centering
\centering \includegraphics[scale=0.46]{\string"eps/HyflowCppArch".pdf}
\caption{HyflowCPP Architecture}
\label{Fig:HyflowCppArch}
\end{minipage}
\end{figure}

\section{Transaction Validation Module}

The purpose of transaction validation module is to provide the transactional consistency and achieve system wide progress. All the DTM logic is performed in this module by extending base class ~\emph{HyflowContext}. Currently by default ~\emph{HyflowContext} is extended as ~\emph{DTLContext} which implements the TFA algorithm. This module validates memory locations and retries the transactional code as needed on commit failure. This module can be configured based on the transaction model used like checkpointing and closed nesting etc. 

To support the DTM protocols this module also provides the ~\emph{lock-table} for object level or word level locking. This table is implemented using high perform concurrent hash-maps of Thread Building Block(TBB)~\cite{willhalm2008putting} library. This module also provides the ~\emph{context-map} to access the context instance using transaction Id. It enable the contention Managers to collect meta data and make updates to different transactional contexts.

This module interfaces with transaction interface module to create the transactional contexts for user applications and provides the object access to user through object access module. It also usages the message interface to perform context specific messaging for commit and validation requests to the remote nodes.

\section{Object Access Module}

Object access module serves multiple purpose in HyflowCpp framework. It provides the copy of distributed objects, object owner information, and performs object version validation and object directory updates. Objects are located using the unique object Id. This module encapsulates a directory lookup protocol to access distributed objects. Currently by default it implements the efficient tracker directory cache coherence protocol. Tracker directory moves the object across nodes and maintains the current owner information on specific tracker node. To access any object this module first finds out the current owner information using object directory, then, it sends the object request to owner node. Owner node replies current copy of object or a null value, in case it got deleted by some other transaction. Tracker directory updates the owner information in tracker node object directory as object moves from one node to another node. Also on object creation or deletion it updates the object directory with owner information. 

Similar to transaction validation module, object access module contains two efficient object maps to support any object access protocol. It provides the ~\emph{local cache} and ~\emph{object directory}. The purpose of Local Cache is to maintain the authoritative copy of objects owned by current node. Meanwhile, directory is utilized by tracker nodes to keep the meta data information, like in case of Tracker Directory, the object owner information. 

Object access module interfaces with two other Modules ~\emph{object validation module} and ~\emph{message interface module}. To provide ~\emph{strong atomicity} HyflowCpp directs all the access to objects through transaction context. Therefore all object requests to object access module come through transaction validation module. Object deletion or publication requests are also made by transaction validation module. In addition to that object access module handles also the object validation request. For all remote object and request serialization and de-serialization Object Access Module interacts with the message interface modules.

\section{Object Serialization Module}

Message serialization and de-serialization is a big challenge in distributed computing using C++. To free user from this cumbersome process HyflowCpp provides a messaging interface and allows developer to add pluggable validation and distribution protocols without worrying about serialization and de-serialization of messages. HyflowCpp provides ~\emph{HyflowMessage} and ~\emph{BaseMessage} classes for this purpose. BaseMessage acts a parents class for any message in HyflowCpp. User can create any new type of message to perform any protocol specific task, just by extending BaseMessage. HyflowMessage class acts as wrapper class for all BaseMessages and its extensions. HyflowMessage contains the information about the BaseMessage wrapped around by it and provides the required information to de-serialize any message.

HyflowMessage provides a standard interface towards network library. All the HyflowMessages are converted in a binary blob and provided to network library communicate over network. In this way network implementation is totally independent of ~\emph{transaction validation module} and ~\emph{object access module}. For serialization and de-serialization of HyflowMessage we use the ~\emph{Boost serialization}~\cite{karlsson2005beyond}. It provides the support for serialize of any type of complex message or object.  

Object serialization module is central part of HyflowCpp framework. It is accessed by all other modules except Transactional Interface Module. Object validation module and object access module use this interface to send transactional messages and object requests, while network manager and message handling interface utilize it to process incoming message and provide responses to upper layer modules.

\section{Network Module}

Network module plays a very vital role in performance of a DTM protocol. A poor implementation and scalability of network module can lead to a poor performance even for an efficient DTM algorithm. In HyflowCpp we provide pluggable support for any network library through a well defined network interface. Currently in HyflowCpp we support two networking libraies: MsgConnect~\cite{MsgConnect:2012} and ZeroMQ~\cite{hintjens2011omq}.  

\begin{figure}
\begin{minipage}[b]{0.9\linewidth}\centering
\centering \includegraphics[scale=0.47]{\string"eps/HyflowCppNetwork".pdf}
\caption{ZeroMQ Network Architecture}
\label{Fig:HyflowCppNetwork}
\end{minipage}
\end{figure}

MsgConnect is a open source library for linux platforms. It provides a high level messaging interface for users. It frees user from socket level message handling and provides a reliable way of communication using TCP protocol~\cite{forouzan2002tcp}. Unfortunately we found some scalability issues in open source implementation. Still, for basic prototyping it can be reliable networking library.

To design a fast and scalable networking solution we use industry standard ZeroMQ library. ZeroMQ is socket level library, but provides very efficient solutions for in-process communication between threads, which makes zeroMQ very suitable for any multi-threaded networking requirement. In Figure~\ref{Fig:HyflowCppNetwork}, we describe our networking architecture designed using ZeroMQ socket library. This architecture design allows configuring architecture to fine tune for desired workload. 

As described in Figure~\ref{Fig:HyflowCppNetwork} any transactional node A and B communicates with each other using ~\emph{forwarder} and ~\emph{catcher} threads. These dedicated threads are responsible for connecting with different nodes using ZeroMQ router~\cite{hintjens2011omq} socket. ZeroMQ router sockets are very useful to communicate with multiple nodes simultaneously. Forwarder threads are responsible for receiving message request from transactional threads and forward it to catcher thread of desired nodes. On other side catcher threads are responsible for receiving message work-load from forwarder thread and assign it to available ~\emph{worker} thread. Worker threads process the message send back reply to catcher thread if required. Catcher thread in turn returns message back to forwarder thread, which conveys it to original requester transactional thread.

ZeroMQ provides four to five times better performance in comparison to MsgConnect. Using multipart messaging of ZeroMQ, we are able to reduce the amount of polling between threads to bare minimum at two sockets. With experiments we found that one forwarder is enough for six to seven transactional threads. On contrast, even one worker per catcher can be sufficient for message processing in some situations. To fine tune these values, we define two ratios ~\emph{zeroMQTFR} i. e., zeroMQ transactional thread to forwarder thread ratio and ~\emph{zeroMQWFR} i. e., zeroMQ worker thread to forwarder thread ratio. It is worth noting that forwarder to catcher ratio is always one, therefore zeroMQWFR also defines the Catcher threads to worker thread ratio. By fine tuning these values user can extract a very high messaging throughput.

\section{Message Processing Module}

Message processing module provides message handling capability to any network library. The major task to this interface is to find a proper handler for any message and support asynchronous messaging. When a user create a new type of message, he/she also creates a proper message handler of this message type. All message handlers are registered by Message handler interface, at network initiation time. Later, when network library receives a request message using the message handler it creates a proper response and replies back as required.

Another important task performed by this module is to support the asynchronous messaging. This module defines a class ~\emph{HyflowMessageFuture}, which allows a transactional thread to send a message asynchronously. A transactional thread can send a message with a HyflowMessageFuture object and proceed with other tasks. Later transaction thread can come to message specific HyflowMessageFuture to wait on the message response. 

This module directly interface with network module and object serialization module. All the messages and there handling is defined in object serialization module. Message and there handler mapping is defined in the message processing module. Network module utilizes this module to find the appropriate handler and provide the asynchronous message response notification to requesting transactional threads.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																									%
%							CHAPTER 5	:	Algorithms						%
%																									%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Transactional Forwarding Algorithm}\label{chap:algorithm}
\markright{Chapter~\ref{chap:algorithm}.
Transactional Forwarding Algorithm
\hfill}

In this chapter we describe Transaction Forwarding Algorithm(TFA) in detail for different transactional models. TFA is a lock-based algorithm with lazy acquisition scheme. It usages an innovative asynchronous distributed clock mechanism to validate the transactions. TFA supports the opacity property~\cite{guerraoui2009semantics} and guarantees  strong progress~\cite{guerraoui2009semantics}. Opacity is a stronger property in comparison to serialization and informally can be described as an extension of the classical database serializability property with the additional requirement that even non-committed transactions are prevented from accessing inconsistent states. Strong progressiveness promises that all non-conflicting transactions will commit and at least one of all conflicting transactions will commit.  
 
Now we present the TFA algorithm in different transactional models and define its main procedures for transaction validation.
 
\section{Flat Nesting}
TFA uses a synchronization variant similar to Lamport~\cite{lamport1978time} mechanism to keep the clocks synchronized. In TFA each distributed node has a local clock $lc$. Each node increases its local clock atomically on a write transaction commit. All the communication between nodes piggyback the nodes local clock. On receiving the messages of remote node, each nodes compares the piggybacked remote node clock with its local clock. Node updates its clock to remote node clock if it is higher than its local clock, otherwise it ignore it. This way all the nodes are kept in synchronization and are able to establish the happens before relationship between object reads. 

Asynchronous distributed clocking enables the TFA to detect any early conflicts in the transaction. Early conflicts in the transaction are detected through a process called Transaction Forwarding, which is performed at the time of object access(read/write). 

\begin{figure}
\begin{minipage}[b]{0.9\linewidth}\centering
\begin{lstlisting}
Context::TransactionForwarding(senderNodeClock) {
	if senderNodeClock > transaction.timeStamp {
		forAll obj in readSet {
			if (currentVersion(obj)>transaction.timeStamp) { 
				transaction.rollback();
				return ;		
			}
		}
		transaction.timeStamp = senderNodeClock ; 
	}
}
\end{lstlisting}
\end{minipage}
\caption{Transaction Forwarding in Flat Nesting}
\label{Fig:FlatTFA}
\end{figure}

\begin{figure}[H]
\begin{minipage}[b]{0.9\linewidth}\centering
\begin{lstlisting}
Context::Commit() {
	forAll obj in writeSet {
		lock = obj.acquireLock()
		if !lock
			rollback
	}
	forAll obj in readSet {
		valid = obj.readValidate();
		if !valid
			rollback
	}
	nodeClock++;
	commitWriteSet();
	forAll obj in transaction.writeSet
		obj.commitValue()
 		obj.setVersion(transaction.timeStamp)
 		obj.releaseLock()
 		if obj.remote then
 			updateOwner(obj)
	forAll obj in transaction.publishSet
		publish(obj)
	forAll obj in transaction.deleteSet
		delete(obj)
}
\end{lstlisting}
\end{minipage}
\caption{Commit in Flat Nesting}
\label{Fig:FlatCommit}
\end{figure}

On start of any transaction, node's local clock is attached to its context as a time stamp $wv$. When any object is accessed by a transaction the sender nodes clock $rc$ is compared against the transactions time stamp $wv$. If sender nodes clock $rc$ is greater than the transactions time stamp $wv$, we verify whether we can move the transaction time stamp $wv$ to $wv'$, where $wv'$ = $rc$. This validation is done by validating all read-set objects version against transaction time stamp $wv$, if they are still less than $wv$, we can safely forward transaction from $wv$ to $wv'$. Figure ~\ref{Fig:FlatTFA} illustrates the Transaction Forwarding procedure.

In Figure~\ref{Fig:FlatCommit} we illustrate the transaction commit process. Transaction commit in TFA is performed similar to two phase commit~\cite{TwoPC:weikum1991principles} protocol. At commit time the transactional node tries to acquire locks on the write-set objects in any appropriate order to avoid deadlocks. Remote object lock requests are send to object owner and if any of locks can not be acquired transaction rollbacks. After successfully acquiring the object locks, transaction tries to re-validate the read-set object. Again if read-set objects validation fails the transaction performs rollback. Once write set locks are acquired and read-set objects are validated it is safe to commit the transaction. In commit process for write transactions local clock is increased atomically. All write-set object version is updated to transaction version. For local objects the updated copies of objects is committed and for remote the change of object ownership is performed. Publish-set and delete set object entries are updated in object access module. After completion of commit process all the write-set object locks are released. 
   
\section{Closed Nesting} 

In closed nesting each transaction attempts to do individual commits, but for inner transactions the commit not visible outside of the enclosing transaction. Inner transactions are allowed to abort independently of their parents. In this way by permitting partial aborts for the inner transactions closed nesting helps to improve performance. In this section we present the modified version of TFA for closed nesting: ~\emph{Transaction Forwarding Alogrithm for closed nesting(TFACN)}.

\begin{figure}[H]
\begin{minipage}[b]{0.9\linewidth}\centering
\begin{lstlisting}
Context::TransactionForwarding(senderNodeClock) {
	if senderNodeClock > transaction.timeStamp {
		contextList = context.fetchContextBranch()
		forAll context in contextList {
			forAll obj in context.readSet {
				if (currentVersion(obj)>transaction.timeStamp) { 
					transaction.rollback();
					return ;
				}		
			}
		}
		transaction.timeStamp = senderNodeClock ; 
	}
}
\end{lstlisting}
\end{minipage}
\caption{Transaction Forwarding in Closed Nesting}
\label{Fig:CloseTFA}
\end{figure} 

To support closed nesting each context maintains a reference to the parent context, for outermost context parent context reference is set to null. In transaction forwarding step we performed transaction forwarding on whole context tree branch instead on just current inner transaction. In Figure~\ref{Fig:CloseTFA} we illustrate the transaction forwarding step for TFACN.

In commit phase TFACN directly merges the context objects to parent context objects for inner transactions. For outermost transaction commit procedure is same as in flat nesting. 

\section{Checkpointing}

Transaction checkpointing saves the transaction execution state at various points. Later on commit failure it enables a transaction to resume from previously saved valid Checkpoint. Checkpointing allows a transaction to abort only required part of transaction for which actually the conflict happens and therefore boosts the performance. Checkpointing can be consider as a extension of closed nesting, where partial aborts can be applied at any point in transaction execution state. In this section we present  the modified the TFA algorithm for checkpointing: ~\emph{Transaction Forwarding Algorithm with checkpointing(TFACP)}.

\begin{figure}[H]
\begin{minipage}[b]{0.9\linewidth}\centering
\begin{lstlisting}
Context::TransactionForwarding(senderNodeClock) {
	if senderNodeClock > transaction.timeStamp {
		forAll obj in readSet {
			if (currentVersion(obj)>transaction.timeStamp) { 
				if transaction.checkPointAvailable {
					removeInvalidatedObjects();
					transaction.resume();
				}else {
					transaction.rollback();
					return ;
				}		
			}
		}
		transaction.timeStamp = senderNodeClock ; 
	}
}
\end{lstlisting}
\end{minipage}
\caption{Transaction Forwarding in Checkpointing}
\label{Fig:CheckPointTFA}
\end{figure} 

To support checkpointing in TFACP we partition read-set, write-set, publish-set and delete-set data on basis of first access Checkpoint. This allows TFACP to identity the conflicting objects and to determine the valid Checkpoint to resume. To save the transaction execution stack state we use the ~\emph{setContext()} and ~\emph{getContext()} functions. Heap objects are maintained in context read-set, write-set and publish-set. 

\begin{figure}
\begin{minipage}[b]{0.9\linewidth}\centering
\begin{lstlisting}
commit::tryResume() {
	if transaction.checkPointAvailable {
		removeInvalidatedObjects();
		releaselocks();
		transaction.resume();
	}else {
		transaction.rollback();
		return ;
	}		
}

Context::Commit() {
	forAll obj in writeSet {
		lock = obj.acquireLock()
		if !lock
			tryResume()
	}
	forAll obj in readSet {
		valid = obj.readValidate()
		if !valid
			tryResume()
	}
	nodeClock++
	commitWriteSet()
	forAll obj in transaction.writeSet
		obj.commitValue()
 		obj.setVersion(transaction.timeStamp)
 		obj.releaseLock()
 		if obj.remote then
 			updateOwner(obj)
	forAll obj in transaction.publishSet
		publish(obj)
	forAll obj in transaction.deleteSet
		delete(obj)
}
\end{lstlisting}
\end{minipage}
\caption{Commit in Checkpointing}
\label{Fig:CheckpointCommit}
\end{figure}

HyflowCpp provides a helper class ~\emph{CheckPointProvider} to create, iterator and maintain the transaction checkpoints.~\emph{HYFLOW{\_}STORE(OBJECT{\_}REF, OBJECT{\_}VALUE)} macro is provided to store any stack or heap object values just before creating checkpoint. These values are automatically are restored on continuing from checkpoint after partial abort.     

In Figure~\ref{Fig:CheckPointTFA} we illustrate the transaction forwarding process for TFACP. Similar to flat nesting we perform transaction forwarding by object validation. But on validation failure, we do not rollback instead resume the transaction from valid checkpoint if available. Before resuming we remove all the invalidated heap objects from the context.

Commit process in TFACP is also modified similar to transaction forwarding. On commit failure in place of restarting transaction we resume from available valid checkpoint. Note that here we are required to release any acquired lock before resuming. Figure~\ref{Fig:CheckPointCommit} illustrate the commit procedure in TFACP. 


\section{Open Nesting}

Open nesting provides the performance improvement by reducing the false memory level conflict. In contrast to closed nesting in open nesting the inner transactions commit on completion are visible globally and corresponding undo action is merged into parent transaction. In case of parent transaction abort the undo action are used to recover from the inner transaction commit. To maintain the memory consistency the inner transaction specific higher level locks are maintained.  

To support open nesting in TFA a concept of abstract locks is introduced, which provided the higher level locks for inner transactions. Unfortunately abstract lock mechanism suffers from the livelocks issue and uses a random back-off mechanism to overcome this issue. TFAON allows user to define the \textit{onAbort} and \textit{onCommit} functions to be performed in case of abort or commit on parent transaction. TFAON acquires abstract locks in inner transactions and releases them in parent transaction on commit or abort. 

\begin{figure}
\begin{minipage}[b]{0.9\linewidth}\centering
\begin{lstlisting}
Benchmark::transaction() {
	Atomic atomicTxn;
	atomicTxn.atomic = Benchmark::transactionAtomic;
	atomicTxn.onAbort = Benchmark::onAbortTxn;
}

Benchmark::transactionAtomic() {
    string txnLock;
	readOperations();
	writeOperations();
	if (nesting == HYFLOW_OPEN_NESTING ) {
		string txnlock;
		__context__.onLockAccess(Benchmark, txnLock, false);	
	}
}	

Context::Commit() {
	forAll obj in writeSet {
		lock = obj.acquireLock()
		if !lock
			rollbackInnerTxn()
	}
	forAll obj in readSet {
		valid = obj.readValidate()
		if !valid
			rollbockInnerTxn()
	}
	
	for abstractLock in abstractLockSet {
		lock = abstractLock.acquireLock()
		if !lock
			rollback();
	}
	nodeClock++
	commitWriteSet()
	forAll obj in transaction.writeSet
		obj.commitValue()
 		obj.setVersion(transaction.timeStamp)
 		obj.releaseLock()
 		if obj.remote then
 			updateOwner(obj)
	forAll obj in transaction.publishSet
		publish(obj)
	forAll obj in transaction.deleteSet
		delete(obj)
		
	if topTransaction {
		releaseAbstractLocks()
	}else {
		mergeAbstractLockToParent()
	}	
}
\end{lstlisting}
\end{minipage}
\caption{Commit in Open Nesting}
\label{Fig:OpenNestingCommit}
\end{figure}

As described in figure~\ref{Fig:OpenNestingCommit} abstract locks can be created by provide unique string name for transaction. Abstract lock are registered in transaction using the \textit{onLockAcess} function. At commit time the registered abstract locks are acquired. Later, abstract locks are either merged to parent context or released if top transaction. In case of aborts rollback is performed by using onAbort function. Once compensating action for related inner transaction is completed abstract locks are released. Compensation action run with higher priority and don't perform random back-off.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																									%
%							CHAPTER 6	:	Experiments						%
%																									%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental Results \& Evaluation}\label{chap:expResults}
\markright{Chapter~\ref{chap:expResults}.
Experimental Results \& Evaluation
\hfill}

In this chapter, the performance of HyflowCPP is compared against the other DTM frameworks using micro-benchmarks and macro-benchmarks. Being first DTM framework in C++, we compare this work with other JVM based competitors like HyflowScala, HyflowJava, GenRSTM and DecentRSTM. We evaluate the performance on multiple micro benchmarks(Bank, List, Binary Search Tree, Skip-list, Hash-table) and  macro benchmarks(Loan, Vacation and Tpcc).   

\section{Test Environment}

Our all experiments are conducted on 48 core machine. Machine contains four AMD Opteron\texttrademark  processors (6164 HE), each with 12 cores running at 1700 MHz, and 16 GB of memory. All experiment are conducted in Linux environment using Ubuntu Linux Server 10.04 LTS 64-bit.  Each core communicates with other core via network loop-back TCP connection, emulating the behavior of 48 distributed nodes. All the process instance of benchmarks are bound to specific core to stop unnecessary context-switch between different cores. Framework dependency libraries \textit{Boost} version 1.40, \textit{ZeroMQ} version 1.30 and \textit{Intel Thread Build Block} version 4.0 are used.  

\section{Flat Nesting}

Flat nesting does not provide any performance incentive over other partial abort models like closed or open nesting. Still, it is very useful to understand benchmarks characteristics and compare the raw performance between different DTM frameworks. We have evaluated the performance with different read ratios (0.2 , 0.5 and 0.8) on nodes from 4 to 48. For concentrating focus on the distributed behavior in experiments we have limited threads on each node to one.  

For performance comparsion, we compare the HyflowCpp with various competitors like HyflowJava~\cite{Saad:2011:HHP:1996130.1996167}, HyflowScala~\cite{turcuhyflow2}, DecentRSTM~\cite{DecentSTM:5470446} and GenRSTM~\cite{GenRSTM:6038614}. We have compared the all the benchmarks already available in competitors with our benchmarks. For some HyflowCPP benchmarks, we have not found any implementation in competitors. We have not spent time in developing benchmarks in competitor frameworks, instead we devoted our time in more important research work. 

\subsection{Micro Benchmarks}

Micro benchmarks which we considered for performance evaluation are Bank, List, Binary Search Tree, Skip-list and Hash-table. In each data structure we convert all coarse grain lock accesses in lock based implementation with \emph{HYFLOW{\_}ATOMIC{\_}START} and \emph{HYFLOW{\_}ATOMIC{\_}END} transactional block. For each experiment we ran 2000 transactions 3 times and took average of their transactional throughput.  

\subsubsection{Bank}

The Bank benchmark simulates a distributed banking system. Each distributed node is initiated with certain number of bank accounts. Each account is created with some initial amount. Bank benchmark supports two operations: total balance and transfer. At the end of experiment a sanity check is performed. Bank benchmark is distributed in nature and allows multiple Transfer operations in parallel on different accounts.

In this experiment, we create 10000 accounts distributed over different nodes. In figure~\ref{Fig:flatBank}, we can observer the performance of the HyflowCPP in comparison to various competitors for different read percents: 20\%, 50\% and 80\%. Bank being distributed in nature allows HyflowCPP performance to linearly scale with increase in numbers of nodes. As we create 10000 accounts over the network, increase in number of nodes does not increase contention.

HyflowCPP performs 3 to 5 times better in comparison to its nearest competitor HyflowScala. In sweet spot comparison by adding lots of threads per core HyflowScala is able to achieve almost equivalent performance to HyflowCPP as showed in figure~\ref{Fig:flatBankSS}. HyflowScala uses very efficient messaging technique based on the actor model, which allows it scale quickly with increase in number of threads.

Poor performance of HyflowJava and  DecentSTM is rooted in their bad implementation. The HyflowJava and HyflowCPP use the same TFA algorithm for concurrency control. But networking support in HyflowJava is very rudimentary and one of the major reason for bad performance. Similarly the DecentSTM and GenRSTM implementation is also not optimized for network messaging. From algorithmic point of view DecentSTM and GenRSTM should be able to perform better in case of high reads as replication allows to access objects locally.

\begin{figure}
\centering
\subfigure[Bank 20\% reads]{
\includegraphics[width=0.75\textwidth]{\string"eps/flat/bank/r20".pdf}
\label{Fig:flatBankR20}
}
%\qquad
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Bank 50\% reads]{
\includegraphics[width=0.75\textwidth]{\string"eps/flat/bank/r50".pdf}
\label{Fig:flatBankR50}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Bank 80\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/bank/r80".pdf}
\label{Fig:flatBankR80}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Bank Sweetspot Comparison]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/bank/sw".pdf}
\label{Fig:flatBankSS}
}
\caption{Flat Nesting: Transactional throughput for Bank}
\label{Fig:flatBank}
\end{figure}

\subsubsection{Linked List}

In this benchmark, we have designed an ordered, singly-linked linear list data structure. We implement the list as set and don't allow the duplicate objects in the list. It helps to control the maximum number of objects in list. It supports two write operations: add and remove, and one read operation: contains. While traversing the list all the read objects are added to transaction readSet, which makes list prone to high contention. Any write operation on traversed node will lead to transaction abort.

For our experiments we initialize the linked list with 50 objects. In figure~\ref{Fig:flatList}, we observe that HyflowCPP performance is 3 to 5 times better in comparison to nearest competitor HyflowScala. HyflowJava performs even worse than HyflowScala. We also find increase in transactional throughput with increase in read percent. We are unable to find any consistent pattern in throughput with increase in number of nodes as contention is very high in list and aborts happen in random order. On average the list throughput decreases as number of nodes are increased, due to higher contention.

\begin{figure}[H]
\centering
\subfigure[Linked list 20\% reads]{
\includegraphics[width=0.85\textwidth]{\string"eps/flat/list/r20".pdf}
\label{Fig:flatListR20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Linked list 50\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/list/r50".pdf}
\label{Fig:flatListR50}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Linked list 80\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/list/r80".pdf}
\label{Fig:flatListR80}
}
\caption{Flat Nesting: Transactional throughput for Linked list}
\label{Fig:flatList}
\end{figure}

\subsubsection{Skip List}

In this benchmark, we design standard Skip-list data structure. Similar to list, in skip list also we don't allow the duplicate values. Skip-list benchmark supports two write operations: add and remove, and one read operation: contains. In Skip-list contention is generally lower than the list as it requires fewer nodes to traverse. 

In our experiments we create the Skip-list with upto 50 objects. In figure~\ref{Fig:flatSkipList}, we can observer that HyflowCPP performance is 3 to 5 times better in comparison to HyflowScala. HyflowJava performs even worse in comparison to HyflowScala. With increase in number of nodes the HyflowCpp performance for 20\% reads decreases in general and for 50\% and 80\%  it increases. Overall due to smaller read set size the Skip-list throughput is better than Linked List benchmark. 

\begin{figure}[H]
\centering
\subfigure[Skiplist 20\% reads]{
\includegraphics[width=0.85\textwidth]{\string"eps/flat/skipList/r20".pdf}
\label{Fig:flatSkipListR20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Skiplist 50\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/skipList/r50".pdf}
\label{Fig:flatSkipListR50}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Skiplist 80\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/skipList/r80".pdf}
\label{Fig:flatSkipListR80}
}
\caption{Flat Nesting: Transactional throughput for Skip-list}
\label{Fig:flatSkipList}
\end{figure}

\subsubsection{Binary Search Tree}
In this benchmark, standard the Binary Search Tree is implemented as a set. It support two write operations: add and remove, and one read operation: contains. BST provides the higher concurrency in comparison to list, as it allows the concurrent read and write operations to go in parallel for different sub-trees. 

In figure~\ref{Fig:flatBst}, we can observer HyflowCpp performs 3 to 5 times better in comparison to HyflowScala. HyflowJava performance remains poor as in previous cases. Due to higher level of concurrency, we can find the increase in throughput for increase in number of nodes for all read ratios. In our experiments we allowed upto 50 objects in BST. Higher concurrency available in BST can be easily understood via its throughput which is upto 2 times better in comparison to Linked List.

\begin{figure}[H]
\centering
\subfigure[BST 20\% reads]{
\includegraphics[width=0.85\textwidth]{\string"eps/flat/bst/r20".pdf}
\label{Fig:flatBstR20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[BST 50\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/bst/r50".pdf}
\label{Fig:flatBstR50}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[BST 80\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/bst/r80".pdf}
\label{Fig:flatBstR80}
}
\caption{Flat Nesting: Transactional throughput for Binary Search Tree}
\label{Fig:flatBst}
\end{figure}

\subsubsection{Hash Table}
In this benchmark, we implement a bucket based standard Hash-table. We don't allows duplicate keys in the Hash-table. It support two write operations: add and remove, and one read operation: find. Hash-table is distributed in nature and allows the concurrent operations on different buckets. Small read-write sets and distributed nature generally leads to very high throughput for Hash-table.

For our experiment we created 2000 Hash-buckets distributed over different nodes. In figure~\ref{Fig:flatHashTable}, we can observer the performance of the HyflowCPP in comparison to various competitors for different read percents: 20\%, 50\% and 80\%. Hash-table being distributed in nature allows HyflowCPP performance to linearly scale with increase in numbers of nodes. As we create 2000 Hash-buckets over the nodes, increase in number of nodes does not increase contention. Due to smaller read-write set size the Hash-table throughput is highest among all the benchmarks.

HyflowCPP performs 4 to 6 times better in comparison to its nearest competitor HyflowScala. HyflowJava performs even worse in comparison to HyflowCPP. In Hash-table contention happens only when different keys try to access same bucket, other wise they can execute in parallel. Due to this reason, we don't see much difference in throughput for different read ratios.

\begin{figure}[H]
\centering
\subfigure[Hash table 20\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/hashTable/r20".pdf}
\label{Fig:flatHashTableR20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Hash table 50\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/hashTable/r50".pdf}
\label{Fig:flatHashTableR50}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Hash table 80\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/hashTable/r80".pdf}
\label{Fig:flatHashTableR80}
}
\caption{Flat Nesting: Transactional throughput for Hash table}
\label{Fig:flatHashTable}
\end{figure}

\subsection{Macro Benchmarks}

Macro Benchmarks are useful to verify the framework capability in real life applications. Macro benchmarks perform lot of operations leading to higher messaging and transaction execution time. In this section we evaluate the performance of Macro Benchmarks Vacation, Loan and TPCC.

\subsubsection{Vacation}

The Vacation benchmark emulates a itinerary planning reservation system. It allows user to make reservations for cars, rooms and flights. It also enables administrators to manipulate the users and update the currently available offers. Each reservation request or offer update request contain 10 queries writing on the 10 distributed objects. 

Vacation executes the reservation request, a low contention operation, as read operations and delete customer and update offers, high contention operations, as write operations. For experiment, in Vacation we create 1000 customers and 3000 cars, 3000 flights and 3000 rooms. In figure~\ref{Fig:flatVacation} we can see  that higher numbers of resources allow the throughput to scale linearly with number of nodes. Due to higher numbers of objects the read operations are more costly. We compare the HyflowCPP performance with HyflowJava for different reads. HyflowCPP perform also 4-5 times better than HyflowJava. 

\begin{figure}[H]
\centering
\subfigure[Vacation 20\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/vacation/r20".pdf}
\label{Fig:flatVacationR20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Vacation 50\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/vacation/r50".pdf}
\label{Fig:flatVacationR50}
}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[Vacation 80\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/vacation/r80".pdf}
\label{Fig:flatVacationR80}
}
\caption{Flat Nesting: Transactional throughput for Vacation}
\label{Fig:flatVacation}
\end{figure}

\subsubsection{Loan}

Loan benchmark simulates the banking situation where loans are provided to user based on the different set of bank accounts. Each account is access in nested fashion and a certain amount of money is borrowed based on the current amount. Loan benchmark also allows the user to get total balance available in the all accounts.

In our experiments, we run the total balance operation as reads and loan operation as write operation. We create total 10000 account over network and access 6 accounts to provide loan or calculate the total balance.
Loan operation calls itself in nested fashion either until all the account are used to borrow a random part of total loan money or total money is gathered.

In figure~\ref{Fig:flatLoan} We compare HyflowCPP performance with two other frameworks HyflowJava and DecentSTM. Its performance in 5 to 10 times better than other frameworks. Throughput for 80\% read is higher than 20\% reads as write dominated experiments suffer from higher contention.. At very low contention level, around 2 to 4 nodes, loan performance is very similar for different read ratios as read operation and write operation access same number of objects. 

\begin{figure}[H]
\centering
\subfigure[Loan 20\% reads]{
\includegraphics[width=0.85\textwidth]{\string"eps/flat/loan/r20".pdf}
\label{Fig:flatLoanR20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Loan 50\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/loan/r50".pdf}
\label{Fig:flatLoanR50}
}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[Loan 80\% reads]{
\includegraphics[width=0.8\textwidth]{\string"eps/flat/loan/r80".pdf}
\label{Fig:flatLoanR80}
}
\caption{Flat Nesting: Transactional throughput for Loan}
\label{Fig:flatLoan}
\end{figure}

\subsubsection{TPCC}

TPC-C benchmark is a very popular yardstick for comparing OLTP performance on various hardware and software configurations. TPC-C simulates a computing environment where a group of users executes transactions against a database. TPC-C benchmark runs five operations in predefined ratio: New order(45\%), Payment(43\%), Delivery(4\%), Order Status(4\%), Stock-level(4\%). Stock-level operation is most heavy operation which require to access 300 to 400 objects in single transaction. 

In our experiment we don't vary the read ratios as in previous benchmarks. Because, TPCC have predefined operation ratio. TPCC access various objects in same repositories, for which a remote procedure call (RPC) based algorithm can be more efficient, because RPC request can be send to remote node, instead of bringing all the objects to requester nodes. In figure~\ref{Fig:flatTPCC}, we can observer that the performance of TPCC at start drops with increase in remote nodes as localization of repository objects reduces. Once almost all the objects are access remotely, throughput start to increase with increase in number of nodes. Due to very large read and write set size TPCC benchmark have lowest throughput among all the tested benchmarks.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{\string"eps/flat/tpcc/tpcc".pdf}
\caption{Flat Nesting: Transactional throughput for TPCC}
\label{Fig:flatTPCC}
\end{figure}

\section{Checkpointing and Closed Nesting}

Checkpoing and closed nesting both provide a method to partial rollback in case of abort. We perform various experiments to understand the benefit of checkpointing and closed nesting over flat nesting. We perform our experiments on Bank, List, Skip-list, and Hash-table benchmarks. We alternated various parameters to understand the conditions in which these models can be useful.

We performed our experiments for different read-ratios: 20\%, 50\%, and 80\% and different node counts: 2, 4, 8 and 16. These two parameters allowed us to vary the contention and abort count. We used less number of objects in our experiment, so that enough aborts occur. Without significant number of aborts measuring the impact of partial abort mechanism is not possible. We also varied number of objects based on number of inner transactions to maintain the same contention level across the experiments. We used different number of inner transaction to understand the impact of transactional length and points of partial aborts in transaction.

We can divide our experiments mainly in two groups. In first group we maintained the same number of objects for all the experiments and changed the other parameters like read ratio, number of nodes and number of inner transaction for different nesting models. In second group, we performed experiments same as in first group, except now we increased the number of objects in each experiment with increase in number of inner transaction. It enabled us to maintain the same contention level for given parameters. We also performed a third experiment on checkpointing to examine the impact of checkpoint granularity on transactional throughput. In our results we represent checkpointing and closed nesting throughput relative to flat nesting.

\subsection{Bank}
In figure~\ref{Fig:cpBankEx1R20}, we present the results for 20\% reads in first group of experiments. In these experiments, we maintain object count constant and increase node count and number of inner transactions. We present throughput for different inner transactions ${1, 2, 5, 10}$ for closed nesting and checkpointing relative to flat nesting. We can observer checkpointing performs better in comparison to flat nesting upto 90\%, whereas closed nesting perform upto 60\% better. We can observer that as we increase the inner transaction count, both nesting models performance deteriorates with increase in contention. At 10 inner transaction actually we perform worse than flat nesting.

In figure~\ref{Fig:cpBankEx2R20}, we present the results for 20\% reads in second group of experiments. In these experiments, we increase object count with increase in number of inner transactions. We keep objects to inner transactions ratio constant, which allows us to maintain contention level with increase in number of inner transactions. We can observer with increase in inner transactions relative performance of closed nesting and checkpointing does not decrease and it is able to maintain similar pattern of improvements as for lower number of inner transactions.

In figure~\ref{Fig:cpBankEx3} we present the relative change in throughput and abort rate for minimum to maximum granularity with inner transactions 1 and 10. We can observer that increase in checkpoint granularity certainly decreases the abort rate, but increase in throughput is only 10\% to 15\% only. Relative low throughput improvement occurs as actually all partial rollback don't incur in transaction commit.

\begin{figure}[H]
\centering
\subfigure[Bank: Relative throughputs with constant object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/bank/Ex1/r20".pdf}
\label{Fig:cpBankEx1R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Bank: Relative throughputs with increasing object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/bank/Ex2/r20".pdf}
\label{Fig:cpBankEx2R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Bank: Throughput and Abort rate w.r.t minimum granularity]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/bank/Ex3/r50".pdf}
\label{Fig:cpBankEx3}
}
\caption{Closed Nesting and Checkpointing: Relative Transactional throughput for Bank}
\label{Fig:cpBank}
\end{figure}

\subsection{Linked List}
In figure~\ref{Fig:cpListEx1R20}, we present the results for 20\% reads in first group of experiments. In these experiments, we maintain object count constant and increase node count and number of inner transactions. We present throughput for different inner transactions ${2, 5, 10}$ for closed nesting and checkpointing relative to flat nesting. We can observer that with lower number of inner transactions checkpointing and closed performs worse in comparison to flat nesting. But at higher inner transaction count both nesting models performance improves. So, we establish that checkpointing and closed nesting are helpful only in case of higher inner transactions for list benchmark.

In figure~\ref{Fig:cpListEx2R20}, we present the results for 20\% reads in second group of experiments. In these experiments, we increase object count with increase in number of inner transactions. For such experiments, we obtain similar pattern as in Figure~\ref{Fig:cpListEx1R20}. Similar pattern can be easily as increasing the list size don't impact on contention level. In list, only one transaction proceeds which locks the nearest node to head. Read set size don't impact on contention level in list.

In figure~\ref{Fig:cpListEx3}, we can observer at very high contention abort rate is not effected by transaction granularity and not throughput.

\begin{figure}[H]
\centering
\subfigure[Linked List: Relative throughputs with constant object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/list/Ex1/r20".pdf}
\label{Fig:cpListEx1R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Linked List: Relative throughputs with increasing object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/list/Ex2/r20".pdf}
\label{Fig:cpListEx2R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Linked List with decreasing granularity]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/list/Ex3/r20".pdf}
\label{Fig:cpListEx3}
}
\caption{Closed Nesting and Checkpointing: Relative Transactional throughput for Linked List}
\label{Fig:cpList}
\end{figure}

\subsection{Skip List}
In figure~\ref{Fig:cpSkiplistEx1R20}, we present the results for 20\% reads in first group of experiments. In these experiments, we maintain object count constant and increase node count and number of inner transactions. We present throughput for different inner transactions ${2, 5, 10}$ for closed nesting and checkpointing relative to flat nesting. We can observer that checkpointing and closed performs worse in comparison to flat nesting for lower numbers of inner transactions. With increase in inner transactions more inner partial rollback points are created and overall performance improves.

In figure~\ref{Fig:cpSkiplistEx2R20}, we present the results for 20\% reads in second group of experiments. In these experiments, we increase object count with increase in number of inner transactions. we can observer that overall improvement for Skip-list by closed nesting and checkpointing actually deteriorates in comparison to first group of experiments. This loss in performance can be explained by increase in contention due to bigger read set size, because with increase in Skip-list size the number of nodes required to access the target node increases in Skip-list.

In figure~\ref{Fig:cpSkiplistEx3},

\begin{figure}[H]
\centering
\subfigure[Skip-list: Relative throughputs with constant object count for different number of inner transactions]{constant
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/skiplist/Ex1/r20".pdf}
\label{Fig:cpSkiplistEx1R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Skip-list: Relative throughputs with increasing object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/skiplist/Ex2/r20".pdf}
\label{Fig:cpSkiplistEx2R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Skip-list: Relative throughputs with increasing object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/skiplist/Ex3/r20".pdf}
\label{Fig:cpSkiplistEx3}
}
\caption{Closed Nesting and Checkpointing: Relative Transactional throughput for Skip-list}
\label{Fig:cpSkiplist}
\end{figure}

\subsection{Hash Table}

In figure~\ref{Fig:cpHashTableEx1R20}, we present the results for 20\% reads in first group of experiments. In these experiments, we maintain object count constant and increase node count and number of inner transactions. We present throughput for different inner transactions ${2, 5, 10}$ for closed nesting and checkpointing relative to flat nesting. We can observer checkpointing performs better in comparison to flat nesting upto 100\%, whereas closed nesting perform upto 90\% better. We can observer that as we increase the inner transaction count towards both nesting models performance improves with increase in partial rollback points. This behavior is contrary to bank where increase in inner transaction deteriorates the performance. In Hash-table only one object is accessed in each transaction, therefore increase inner transactions actually helps to improve performance due increase in partial rollback points, instead of decrease due to increased object contention.

In figure~\ref{Fig:cpHashTableEx2R20} we present the results for 20\% reads in second group of experiments. In these experiments, we increase object count with increase in number of inner transactions. In these experiments we keep objects to inner transactions ratio constant, which allows us to stop increase in contention level with increase in number of inner transactions. We can observer with increase in inner transactions relative performance of closed nesting and checkpointing does not decrease and it is able to maintain similar pattern improvements as for lower number of inner transactions.

In figure~\ref{Fig:cpHashTableEx3} we present the relative change in throughput and abort rate for minimum to maximum granularity with inner transactions 1 and 10. We can observer that increase in checkpoint granularity decreases the abort rate 30\% to 70\%, and increase in throughput is only 20\% to 50\%. Relative low improvement occurs as actually all partial rollback don't incur in transaction commit.

\begin{figure}[H]
\centering
\subfigure[Hash-table: Relative throughputs with constant object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/hashTable/Ex1/r20".pdf}
\label{Fig:cpHashTableEx1R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Hash-table: Relative throughputs with increasing object count for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/hashTable/Ex2/r20".pdf}
\label{Fig:cpHashTableEx2R20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Hash-table: Throughput and Abort rate w.r.t minimum granularity]{
\includegraphics[width=0.7\textwidth]{\string"eps/checkpointing/hashTable/Ex3/r20".pdf}
\label{Fig:cpHashTableEx3}
}
\caption{Closed Nesting and Checkpointing: Relative Transactional throughput for Hash-table}
\label{Fig:cpHashTable}
\end{figure}

\section{Open Nesting}

The aim of our experiments for open nesting is to analyze its performance under different the parameters  and identify the workload conditions which are best for open nested transactions. We compare the improvements achieved by open nesting with flat and closed nesting. We perform our experiments on configurable benchmarks Hash-table, Skip-list, and Binary Search Tree.

We test our open nesting implementation with varying various parameters like read ratio, number of transactional nodes, and number of inner transactions. We perform all these experiments for flat nesting, closed nesting and open nesting. In each experiment, we collect various meta data like committed/aborted transactions, committed/aborted sub-transactions (closed and open nesting), committed/aborted compensating/commit actions (open nesting only) and waiting time after aborted (sub-)transactions (for exponential back-off).

We represent our results for each benchmark three different plots. In first plot we present the relative throughput improvement achieved by open nesting and closed nesting relative to flat nesting for different number of inner transaction per transaction. Second plot describes the impact of different read to write ratios on open and closed nesting with respect to flat nesting. In last plot provide the total time spent by open nested transaction in various actions like commit, abort, abort compensation, and back-off after repeated aborts. 

\subsection{Hash Table}

In figure~\ref{Fig:onHashTableTrpR20}, we present the open and closed nesting throughput relative to flat nesting for different number of inner transactions for 300 bucket objects. We can observer that as we increase the number of inner transactions open nesting performance improves as amount of contention increases. But at very higher contention for 8 inner transaction performance actually falls with increased abstract lock contention. 

In figure~\ref{Fig:onHashTableReads}, we present the open and closed nesting throughput relative to flat nesting for different read ratios for 3 inner transactions. We find increase in throughput with increase in read ratio due to decreased abstract lock contention. It is worth noting we have observered very contention in this experiment between 200\% to 4000\%. 

In figure~\ref{Fig:onHashTableTime}, we examine the time split of open nested transactions. We can observer 
that with increase number of nodes the commit time increase slowly, but increase in total sub-transaction time and abort compensation time is high. Backoff time also increases, but it constitute very small part of total time spent.  

In figure~\ref{Fig:onHashTableObjs}, we present the open and closed nesting throughput relative to flat nesting for different number of inner transaction for 1500 bucket objects.

\begin{figure}[H]
\centering
\subfigure[Hash-table: Open and closed nesting throughput relative to flat nesting with 300 buckets for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/open/hashTable/trp/r20".pdf}
\label{Fig:onHashTableTrpR20}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Hash-table: Open and closed nesting throughput relative to flat nesting for read ratios]{
\includegraphics[width=0.7\textwidth]{\string"eps/open/hashTable/reads/inTx3".pdf}
\label{Fig:onHashTableReads}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Hash-table: Open and closed nesting throughput relative to flat nesting with 1500 buckets for different number of inner transactions]{
\includegraphics[width=0.7\textwidth]{\string"eps/open/hashTable/objs/r50".pdf}
\label{Fig:onHashTableObjs}
}
\end{figure}
\begin{figure}[H]
\centering
\subfigure[Hash-table: Execution time split for open nested transaction]{
\includegraphics[width=0.7\textwidth]{\string"eps/open/hashTable/time/r20".pdf}
\label{Fig:onHashTableTime}
}
\caption{Open Nesting: Relative Transactional throughput and execution time split for Hash-table}
\label{Fig:cpHashTable}
\end{figure}

\subsection{Skip List}

To Be Added 

\subsection{Binary Search Tree}
that
To Be Added

\section{Summary}

In summary we can see HyflowCPP provides a very high throughput in comparison to its competitors.
To be Added.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																									%
%							CHAPTER 7	:	Conclusion						%
%																									%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion and Future Work}\label{chap:conclusion}
\markright{Chapter~\ref{chap:conclusion}.
Conclusion and Future Work
\hfill}

In this thesis we presented a C++ DTM framework: HyflowCPP. In HyflowCPP, we provided a first ever generic programming interface based DTM abstraction in C++ with pluggable support for different concurrency control algorithms, directory lookup protocols, contention management policies, and network communication protocols.
We supported various transaction feature like strong atomicity, flat nesting, closed nesting, open nesting, and checkpointing to export high programmability and good performance.

Using cache coherence Transaction Forwarding Algorithm for concurrency control, HyflowCPP provides excellent performance gains over other JVM based contemporary frameworks like GenRSTM, DecentRSTM, HyflowJava, and HyflowScala. HyflowCPP scales smoothly with increase in number of objects, read-write ratio, and nodes. We performed experiments on 8 different micro- and macro-benchmarks. Our results for flat nesting show that HyflowCPP outperforms its nearest competitor HyflowScala by as much as 6 times. Other competitors like GenRSTM, DecentRSTM and HyflowJava perform much worse in comparison to HyflowCPP. We attribute HyflowCPP high performance to its efficient ZeroMQ based networking support and optimized C++ based implementation. 

We performed various interesting experiments related to partial abort model using HyflowCPP and achieved high  gains over flat nesting. Our closed nesting implementation outperformed flat nesting by as much as 90\%. Similarly using efficient checkpointing approach, we achieved upto 100\% performance improvement over high flat nesting and upto 50\% over closed nesting. Using configurable checkpointing support in HyflowCPP, we demonstrated that maximum granularity checkpointing is not much useful in case of very high abort rate. For first time illustrated that efficient checkpointing can provide higher performance gain in comparison to closed nesting. Our open nesting based experiments qualified the performance results of previous studies and achieved higher improvements. We achieved upto 140\% improvement using open nesting over flat nesting and over 90\% improvement over closed nesting. 



\section{Future Work}

Several directions exist for future work. These include the following:

\begin{enumerate}
\item HyflowCPP does not support some advanced DTM features such as conditional synchronization and 
irrevocable transactions~\cite{welc2008irrevocable}, which can be immediate directions for further work.
\item HyflowCPP does not support any replication based protocol such as tree-quorum~\cite{Zhang:2011:QRF:2183536.2183539}, which will enable the DTM support failure prone links.
\item HyflowCPP based DTM support can be utilized in a production application like in Hadoop to understand the feasibility of DTM in production class software code.
\item To make HyflowCPP programming interface more transparent to programmer, specially for the advance nesting feature, a compiler support can be added. 
\end{enumerate}

\newpage
\markright{Bibliography \hfill}

\bibliographystyle{abbrv}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{BibTex/all}

\end{document}

